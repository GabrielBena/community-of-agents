{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Self Contained Model of the Funcspec Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import EMNIST\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm as tqdm_n\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torchviz import make_dot\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import DataLoader\n",
    "from copy import copy, deepcopy\n",
    "from tqdm.notebook import tqdm, trange\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = [\n",
    "    \"xkcd:cloudy blue\",\n",
    "    \"xkcd:gray\",\n",
    "    \"xkcd:orange\",\n",
    "    \"xkcd:dark seafoam\",\n",
    "    \"xkcd:purple\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from community.data.datasets import get_datasets_symbols\n",
    "from community.utils.plotting import plot_grid, create_gifs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = True\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "batch_size = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Data: 100%|██████████| 60000/60000 [00:01<00:00, 46720.00it/s]\n",
      "Generating Data: 100%|██████████| 10000/10000 [00:00<00:00, 47663.52it/s]\n"
     ]
    }
   ],
   "source": [
    "n_classes = 50\n",
    "\n",
    "data_config = {\n",
    "    \"data_size\": (30000, 5000),\n",
    "    \"nb_steps\": 50,\n",
    "    \"n_symbols\": n_classes - 1,\n",
    "    \"symbol_type\": \"0\",\n",
    "    \"input_size\": 50,\n",
    "    \"static\": True,\n",
    "    \"double_data\": False,\n",
    "}\n",
    "\n",
    "if data_config[\"static\"]:\n",
    "    data_config[\"nb_steps\"] = 6\n",
    "    data_config[\"data_size\"] = [d * 2 for d in data_config[\"data_size\"]]\n",
    "\n",
    "if not data_config[\"double_data\"]:\n",
    "    n_classes //= 2\n",
    "\n",
    "n_bits = np.ceil(np.log2(n_classes)).astype(int)\n",
    "loaders, datasets = get_datasets_symbols(data_config, batch_size, use_cuda, plot=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task_target(target, task):\n",
    "    tasks = task.split(\"_\")\n",
    "    print(tasks)\n",
    "    try:\n",
    "        task = int(tasks[-1])\n",
    "        target = target[:, task]\n",
    "    except ValueError:\n",
    "        \"continue\"\n",
    "    if \"parity\" in tasks:\n",
    "        target = parity_task(target)\n",
    "    elif \"count\" in tasks:\n",
    "        if \"max\" in tasks:\n",
    "            target = symbol_count(target)\n",
    "        elif \"equal\" in tasks:\n",
    "            target = symbol_count(target) != 0\n",
    "    elif \"nonzero\" in tasks:\n",
    "        target = target > 0\n",
    "    elif \"none\" in tasks:\n",
    "        print(target.shape)\n",
    "        target = target.T\n",
    "        print(target.shape)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    return target.type(torch.LongTensor).to(target.device)\n",
    "\n",
    "\n",
    "from community.data.tasks import get_task_target\n",
    "\n",
    "\n",
    "def parity_task(target):\n",
    "\n",
    "    parity = 1 - target.sum(-1) % 2\n",
    "    parity_target = torch.where(parity.bool(), target[:, 0], target[:, 1])\n",
    "    return parity_target\n",
    "\n",
    "\n",
    "def new_parity_task(target):\n",
    "    parity = 1 - target.sum(-1) % 2\n",
    "    equal = target[:, 0].eq(target[:, 1])\n",
    "    parity_target = torch.where(parity.bool(), target[:, 0], target[:, 1])\n",
    "    parity_target = torch.where(\n",
    "        equal, torch.full_like(parity_target, n_classes), parity_target\n",
    "    )\n",
    "\n",
    "    return parity_target\n",
    "\n",
    "\n",
    "def process_data(data, flatten=True, device=device):\n",
    "\n",
    "    if len(data.shape) == 5:\n",
    "        data = data.permute(1, 2, 0, 3, 4)\n",
    "    else:\n",
    "        data = data.transpose(0, 1)\n",
    "    if flatten:\n",
    "        data = data.flatten(start_dim=-2)\n",
    "    else:\n",
    "        data = data.unsqueeze(-3)\n",
    "\n",
    "    return data.float().to(device)\n",
    "\n",
    "\n",
    "def get_data(task=None, flatten=True, device=device):\n",
    "    data, target = next(iter(loaders[1]))\n",
    "    print(data.shape)\n",
    "    data = process_data(data, flatten=flatten)\n",
    "    if task:\n",
    "        target = get_task_target(target, task)\n",
    "\n",
    "    return data, target.float().to(device)\n",
    "\n",
    "\n",
    "def symbol_count(target):\n",
    "    new_target = torch.where(target.argmax(-1).bool(), target[:, 1], target[:, 0])\n",
    "    new_target[target[:, 0] == target[:, 1]] = 0\n",
    "    return new_target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb0c7f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"sum\"\n",
    "get_all_targets = lambda: torch.cat([t for _, t in loaders[1]])\n",
    "\n",
    "if False:\n",
    "\n",
    "    all_targets = get_all_targets()\n",
    "    uniques, unique_counts = all_targets.unique(dim=0, return_counts=True)\n",
    "    task_t = get_task_target(all_targets, task)\n",
    "    task_t.unique(dim=0, return_counts=True), (all_targets[:, 0] == task_t).unique(\n",
    "        dim=0, return_counts=True\n",
    "    ), (all_targets[:, 1] == task_t).unique(dim=0, return_counts=True)\n",
    "    digits_in = lambda d1, d2: (torch.tensor([d1, d2]) == uniques).all(1).any()\n",
    "    digits_idx = (\n",
    "        lambda d1, d2: (torch.tensor([d1, d2]) == uniques).all(1).float().argmax()\n",
    "    )\n",
    "    counts = np.zeros((n_classes + 1, n_classes + 1))\n",
    "    targets = np.zeros((n_classes + 1, n_classes + 1), dtype=object)\n",
    "\n",
    "    for d1 in range(n_classes):\n",
    "        counts[d1, -1] = (\n",
    "            (all_targets[:, 0] == task_t)[all_targets[:, 0] == d1]\n",
    "        ).sum()  # unique_counts[(uniques == d1)[:, 0]].sum()\n",
    "        targets[d1, -1] = str(counts[d1, -1])\n",
    "        for d2 in range(n_classes):\n",
    "            if digits_in(d1, d2):\n",
    "                counts[d1, d2] = unique_counts[digits_idx(d1, d2)]\n",
    "                targets[\n",
    "                    d1, d2\n",
    "                ] = f\"{get_task_target(uniques, task)[digits_idx(d1, d2)].cpu().data.item()} | {unique_counts[digits_idx(d1, d2)]}\"\n",
    "            else:\n",
    "                counts[d1, d2] = -0.1\n",
    "                targets[d1, d2] = \"X\"\n",
    "            counts[-1, d2] = (\n",
    "                (all_targets[:, 1] == task_t)[all_targets[:, 1] == d2]\n",
    "            ).sum()  # unique_counts[(uniques == d2)[:, 1]].sum()\n",
    "            targets[-1, d2] = str(counts[-1, d2])\n",
    "\n",
    "    counts[-1, -1] = unique_counts.sum().cpu().data.item()\n",
    "    try:\n",
    "        d0_count = (all_targets[:, 0] == task_t).unique(dim=0, return_counts=True)[1][1]\n",
    "        print((all_targets[:, 0] == task_t).unique(dim=0, return_counts=True))\n",
    "    except IndexError:\n",
    "        d0_count = 0\n",
    "    try:\n",
    "        d1_count = (all_targets[:, 1] == task_t).unique(dim=0, return_counts=True)[1][1]\n",
    "    except IndexError:\n",
    "        d1_count = 0\n",
    "\n",
    "    targets[-1, -1] = str(f\"D0 : {d0_count} \\n\" f\"D1 : {d1_count}\")\n",
    "\n",
    "    plt.figure(figsize=(5, 5), dpi=150)\n",
    "    ax = sns.heatmap(\n",
    "        counts, cmap=\"inferno\", annot=targets, annot_kws={\"fontsize\": 7}, fmt=\"s\"\n",
    "    )\n",
    "    ax.set_title(\"Number of examples and global targets\")\n",
    "\n",
    "    ax.set_xlabel(\"Digit received by Agent 1\")\n",
    "    ax.set_ylabel(\"Digit received by Agent 0\")\n",
    "    ax.set_xticklabels([str(i) for i in range(n_classes)] + [\"dig=global\"], fontsize=7)\n",
    "    ax.set_yticklabels([str(i) for i in range(n_classes)] + [\"dig=global\"], fontsize=7)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # loaders, datasets = get_datasets_symbols(data_config, batch_size, use_cuda)\n",
    "    all_targets = get_all_targets()\n",
    "    task_t = get_task_target(all_targets, task)\n",
    "    all_targets[:, 1].unique(return_counts=True), task_t.unique(return_counts=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 6, 2, 50, 50])\n",
      "torch.Size([6, 2, 256, 1, 50, 50])\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    datasets[0].regenerate = False\n",
    "    data, target = get_data(flatten=False)\n",
    "    print(data.shape)\n",
    "    create_gifs(\n",
    "        data[:, 0, :, 0, :, :],\n",
    "        target[:, 0],\n",
    "        \"symbols\",\n",
    "        data_config[\"input_size\"],\n",
    "        \"none\",\n",
    "        data_config[\"double_data\"],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmoothStep(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Modified from: https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(aux, x, thr=0):\n",
    "        aux.save_for_backward(x)\n",
    "        return (x >= thr).type(x.dtype)\n",
    "\n",
    "    def backward(aux, grad_output):\n",
    "        # grad_input = grad_output.clone()\n",
    "        (input,) = aux.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input <= -0.5] = 0\n",
    "        grad_input[input > 0.5] = 0\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "smooth_step = SmoothStep().apply\n",
    "sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "class SurrGradSpike(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Here we implement our spiking nonlinearity which also implements\n",
    "    the surrogate gradient. By subclassing torch.autograd.Function,\n",
    "    we will be able to use all of PyTorch's autograd functionality.\n",
    "    Here we use the normalized negative part of a fast sigmoid\n",
    "    as this was done in Zenke & Ganguli (2018).\n",
    "    \"\"\"\n",
    "\n",
    "    scale = 100.0  # controls steepness of surrogate gradient\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, thr=0):\n",
    "        \"\"\"\n",
    "        In the forward pass we compute a step function of the input Tensor\n",
    "        and return it. ctx is a context object that we use to stash information which\n",
    "        we need to later backpropagate our error signals. To achieve this we use the\n",
    "        ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        out = -torch.ones_like(input)\n",
    "        out[input > thr] = 1.0\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor we need to compute the\n",
    "        surrogate gradient of the loss with respect to the input.\n",
    "        Here we use the normalized negative part of a fast sigmoid\n",
    "        as this was done in Zenke & Ganguli (2018).\n",
    "        \"\"\"\n",
    "        (input,) = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad = grad_input / (SurrGradSpike.scale * torch.abs(input) + 1.0) ** 2\n",
    "        return grad\n",
    "\n",
    "\n",
    "# here we overwrite our naive spike function by the \"SurrGradSpike\" nonlinearity which implements a surrogate gradient\n",
    "super_spike = SurrGradSpike.apply\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_padding(S, K, W):\n",
    "    return int(((S - 1) * W - S + K) / 2)\n",
    "\n",
    "\n",
    "def init_weights(net, scale):\n",
    "    for n, p in net.named_parameters():\n",
    "        try:\n",
    "            torch.nn.init.xavier_normal_(p)\n",
    "        except ValueError:\n",
    "            \"continue\"\n",
    "        p.data *= scale\n",
    "\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dims,\n",
    "        use_conv=False,\n",
    "        use_bottleneck=False,\n",
    "        dropout=0.0,\n",
    "        w_scale=1.0,\n",
    "        n_layers=1,\n",
    "        dual_readout=False,\n",
    "        linear_ag=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dims = dims\n",
    "        assert not (use_conv and linear_ag)\n",
    "\n",
    "        self.linear_ag = linear_ag\n",
    "\n",
    "        if not linear_ag:\n",
    "\n",
    "            if use_conv:\n",
    "                # K, S, W = 3, 1, data_config['input_size']\n",
    "                # P = get_padding(S, K, W)\n",
    "                convs = []\n",
    "                channels = [1, 4]\n",
    "                kernels = [5, 3]\n",
    "                pools = [2, 2]\n",
    "                feature_height, feature_width = [data_config[\"input_size\"]] * 2\n",
    "                for i, (K, P, n_in, n_out) in enumerate(\n",
    "                    zip(kernels, pools, channels[:-1], channels[1:])\n",
    "                ):\n",
    "\n",
    "                    convs.append(nn.Conv2d(n_in, n_out, K, 1, padding=\"same\"))\n",
    "                    convs.append(nn.MaxPool2d(P))\n",
    "                    convs.append(nn.ReLU())\n",
    "                    if dropout > 0.0:\n",
    "                        convs.append(nn.Dropout(dropout))\n",
    "\n",
    "                convs.append(nn.Flatten())\n",
    "                self.conv = nn.Sequential(*convs)\n",
    "\n",
    "                # self.conv = nn.Sequential(nn.Conv2d(1, 8, K, S, P), nn.Flatten())\n",
    "\n",
    "                dummy_data = torch.zeros(\n",
    "                    [1] + [channels[0]] + [data_config[\"input_size\"]] * 2\n",
    "                )\n",
    "                dummy_out = self.conv(dummy_data)\n",
    "                print(dummy_out.shape)\n",
    "                self.dims[0] = dummy_out.shape[-1]\n",
    "            else:\n",
    "                self.conv = None\n",
    "\n",
    "            self.use_bottleneck = use_bottleneck\n",
    "            self.linear_ag = linear_ag\n",
    "\n",
    "            self.n_layers = n_layers\n",
    "            if data_config[\"static\"]:\n",
    "                self.cell = nn.RNN(\n",
    "                    dims[0], dims[1], n_layers, bias=False, batch_first=False\n",
    "                )\n",
    "            else:\n",
    "                self.cell = nn.GRU(\n",
    "                    dims[0], dims[1], n_layers, bias=False, batch_first=False\n",
    "                )\n",
    "\n",
    "            self.dropout = nn.Dropout(dropout) if dropout > 0 else None\n",
    "\n",
    "            n_in_readout = dims[1]\n",
    "\n",
    "            if use_bottleneck:\n",
    "                n_bot = 5\n",
    "                self.bottleneck = nn.Sequential(\n",
    "                    nn.Linear(n_in_readout, n_bot, bias=False), nn.ReLU()\n",
    "                )\n",
    "                self.bottleneck.out_features = n_bot\n",
    "                readout = [nn.Linear(n_bot, dims[-1], bias=False)]\n",
    "            else:\n",
    "                # self.readout = nn.Sequential(nn.Linear(n_in_readout, dims[-1], bias=False), nn.Softmax())\n",
    "                readout = [nn.Linear(n_in_readout, dims[-1], bias=False)]\n",
    "\n",
    "        else:\n",
    "            readout = [nn.Linear(dims[0], dims[-1], bias=False)]\n",
    "\n",
    "        self.dual_readout = dual_readout\n",
    "        if dual_readout:\n",
    "            readout.append(deepcopy(readout[0]))\n",
    "\n",
    "        self.readout = nn.ModuleList(readout)\n",
    "\n",
    "        init_weights(self, w_scale)\n",
    "\n",
    "        # self.readout = nn.Linear(dims[1], dims[2], bias=False)\n",
    "\n",
    "    def forward(self, input, state=None, connections=0):\n",
    "\n",
    "        if not self.linear_ag:\n",
    "\n",
    "            if self.conv:\n",
    "                input = self.conv(input)\n",
    "\n",
    "            if len(input.shape) < 3:\n",
    "                input = input.unsqueeze([0])\n",
    "\n",
    "            if state is None:\n",
    "                out, h = self.cell(input)\n",
    "            else:\n",
    "                if self.n_layers > 1:\n",
    "                    h = torch.stack([state[0] + connections, *state[1:]])\n",
    "                else:\n",
    "                    h = state + connections\n",
    "                out, h = self.cell(input, h)\n",
    "\n",
    "            if self.dropout:\n",
    "                out = self.dropout(out)\n",
    "\n",
    "            if self.use_bottleneck:\n",
    "                out = self.bottleneck(out[0]).unsqueeze(0)\n",
    "\n",
    "        else:\n",
    "\n",
    "            out = input.unsqueeze(0)\n",
    "            h = torch.tensor([0])\n",
    "\n",
    "        out = torch.stack([r(out[0]) for r in self.readout])\n",
    "\n",
    "        return out, h\n",
    "\n",
    "\n",
    "class Connection(nn.Linear):\n",
    "    def __init__(self, dims, p, binarize=False, w_scale=1.0):\n",
    "\n",
    "        super().__init__(dims[0], dims[1], bias=False)\n",
    "\n",
    "        init_weights(self, w_scale)\n",
    "\n",
    "        self.sparsity = p\n",
    "        n_in, n_out = dims\n",
    "        self.nb_non_zero = int(p * n_in * n_out)\n",
    "\n",
    "        w_mask = self.init_mask(n_in, n_out)\n",
    "\n",
    "        self.register_buffer(\"w_mask\", w_mask)\n",
    "        self.binarize = binarize\n",
    "\n",
    "        assert (\n",
    "            w_mask.sum() == self.nb_non_zero\n",
    "        ), f\"Number of nonzero connection is {w_mask.sum()}, expected {self.nb_non_zero}\"\n",
    "\n",
    "    def init_mask(self, n_in, n_out):\n",
    "        w_mask = np.zeros((n_in, n_out), dtype=bool)\n",
    "        ind_in, ind_out = np.unravel_index(\n",
    "            np.random.choice(np.arange(n_in * n_out), self.nb_non_zero, replace=False),\n",
    "            (n_in, n_out),\n",
    "        )\n",
    "        if (\n",
    "            len(np.unique(ind_in)) == 1 or len(np.unique(ind_in)) == 1\n",
    "        ) and self.nb_non_zero > 1:\n",
    "            return self.init_mask(n_in, n_out)\n",
    "        else:\n",
    "            print(ind_in, ind_out)\n",
    "            w_mask[ind_in, ind_out] = True\n",
    "            w_mask = torch.tensor(w_mask)\n",
    "            return w_mask\n",
    "\n",
    "    def forward(self, input):\n",
    "        out = F.linear(input, self.weight * self.w_mask)\n",
    "        if self.nb_non_zero > 0:\n",
    "            assert (out != 0).float().sum(\n",
    "                -1\n",
    "            ).max() <= self.nb_non_zero, f\"{(out != 0).float().sum(-1).max()} non zero connections, expected {self.nb_non_zero} !\"\n",
    "        if self.binarize:\n",
    "            out = super_spike(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def binary_conn(target, ag):\n",
    "    encoding = []\n",
    "    encoded_target = target[:, ag].clone().detach()\n",
    "    for d in range(n_bits - 1, -1, -1):\n",
    "        encoding.append(torch.div(encoded_target, 2**d, rounding_mode=\"floor\"))\n",
    "        encoded_target -= (\n",
    "            torch.div(encoded_target, 2**d, rounding_mode=\"floor\") * 2**d\n",
    "        )\n",
    "    return torch.stack(encoding, -1)\n",
    "\n",
    "\n",
    "class DroupoutMasked(nn.Dropout):\n",
    "    # Dropout mask that can't put nonzero connections at zero\n",
    "    def __init__(self, conn_mask, p: float = 0.5, inplace: bool = False) -> None:\n",
    "        super().__init__(p, inplace)\n",
    "\n",
    "        # True where connections are nonzero else False\n",
    "        self.register_buffer(\"conn_mask\", conn_mask)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_temp = super().forward(x)\n",
    "\n",
    "        conn_mask = self.conn_mask.expand_as(x_temp)\n",
    "\n",
    "        # Final dropout mask, True where not dropped or connections are nonzero\n",
    "        mask = (x_temp != 0) | (conn_mask)\n",
    "\n",
    "        return x * mask\n",
    "\n",
    "\n",
    "class Ensemble(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dims,\n",
    "        p,\n",
    "        use_conv=False,\n",
    "        binary_con=False,\n",
    "        use_bottleneck=False,\n",
    "        dual_readout=False,\n",
    "        common_readout=False,\n",
    "        dropout=0.0,\n",
    "        w_scale=0.1,\n",
    "        n_layers=1,\n",
    "        linear_ag=False,\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "        self.n_in, self.n_hid, self.n_out = dims\n",
    "        self.n_layers = n_layers\n",
    "        self.connections = nn.ModuleList(\n",
    "            [Connection([dims[1]] * 2, p, binary_con, w_scale) for _ in range(2)]\n",
    "        )\n",
    "\n",
    "        self.conn_masks = [c.w_mask for c in self.connections]\n",
    "\n",
    "        self.nonzero_received = [torch.where(c)[0] for c in self.conn_masks][::-1]\n",
    "        self.nonzero_sent = [torch.where(c)[1] for c in self.conn_masks]\n",
    "\n",
    "        self.agents = nn.ModuleList(\n",
    "            [\n",
    "                Agent(\n",
    "                    dims,\n",
    "                    use_conv,\n",
    "                    use_bottleneck,\n",
    "                    dropout,\n",
    "                    w_scale,\n",
    "                    n_layers,\n",
    "                    dual_readout[0],\n",
    "                    linear_ag,\n",
    "                )\n",
    "                for i in range(2)\n",
    "            ]\n",
    "        )[::-1]\n",
    "        self.is_community = True\n",
    "\n",
    "        self.use_common_readout = common_readout\n",
    "        self.dual_readout = dual_readout[1]\n",
    "        if common_readout:\n",
    "            readout = [nn.Linear(2 * dims[-2], dims[-1])]\n",
    "            if self.dual_readout:\n",
    "                readout.append(deepcopy(readout[0]))\n",
    "            self.common_readout = nn.ModuleList(readout)\n",
    "\n",
    "    def forward(self, input, forced_conns=None):\n",
    "\n",
    "        self.min_t_comms = data_config[\"nb_steps\"] // 2\n",
    "\n",
    "        states, conns = [[None] for _ in range(2)], [[] for _ in range(2)]\n",
    "        outputs = [[] for _ in range(2)] if not self.use_common_readout else []\n",
    "\n",
    "        for t, t_input in enumerate(input):\n",
    "            for ag, agent in enumerate(self.agents):\n",
    "\n",
    "                ag_input = t_input[ag]\n",
    "\n",
    "                if t >= self.min_t_comms:\n",
    "                    if forced_conns is not None:\n",
    "                        input_connect = forced_conns[:, ag]\n",
    "                    else:\n",
    "                        try:\n",
    "                            input_connect = self.connections[1 - ag](\n",
    "                                states[1 - ag][-1][-1]\n",
    "                            )\n",
    "                        except (IndexError, RuntimeError) as e:  # Linear ag\n",
    "                            input_connect = torch.tensor([0])\n",
    "                else:\n",
    "                    input_connect = 0\n",
    "\n",
    "                try:\n",
    "                    out, h = agent(ag_input, states[ag][-1], input_connect)\n",
    "                except IndexError:  # Linear agent\n",
    "                    out, h = agent(ag_input)\n",
    "\n",
    "                states[ag].append(h)\n",
    "                if not self.use_common_readout:\n",
    "                    outputs[ag].append(out)\n",
    "                conns[ag].append(input_connect)\n",
    "\n",
    "            if self.use_common_readout:\n",
    "                out = torch.stack(\n",
    "                    [\n",
    "                        r(torch.cat([s[-1] for s in states], -1))[0]\n",
    "                        for r in self.common_readout\n",
    "                    ]\n",
    "                )\n",
    "                outputs.append(out)\n",
    "\n",
    "        if not self.use_common_readout:\n",
    "            outputs = torch.stack([torch.cat(o) for o in outputs], 1)\n",
    "        else:\n",
    "            outputs = torch.stack(outputs, 0)\n",
    "\n",
    "        if self.n_layers > 1:\n",
    "            states = torch.stack(\n",
    "                [torch.stack([s[-1] for s in st[1:]]) for st in states], 1\n",
    "            )\n",
    "        else:\n",
    "            states = torch.stack([torch.cat(s[1:]) for s in states], 1)\n",
    "\n",
    "        conns = torch.stack([torch.stack(c[self.min_t_comms :]) for c in conns], 1)\n",
    "\n",
    "        # print((outputs[-1][1] == outputs[-1][1]).all())\n",
    "\n",
    "        return outputs, states, conns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decisions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_decision(outputs, decision_params, target=None):\n",
    "    temporal_decision, agent_decision = decision_params\n",
    "\n",
    "    if temporal_decision == \"last\":\n",
    "        outputs = outputs[-1]\n",
    "    elif temporal_decision == \"sum\":\n",
    "        outputs = outputs.sum(0)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    agent_decisions = agent_decision.split(\"_\")\n",
    "\n",
    "    try:\n",
    "        deciding_ags = int(agent_decisions[-1])\n",
    "        outputs = outputs[deciding_ags]\n",
    "        deciding_ags = torch.ones(outputs.shape[0]) * deciding_ags\n",
    "        return outputs, deciding_ags\n",
    "\n",
    "    except ValueError:\n",
    "\n",
    "        if agent_decision == \"max\":\n",
    "            device = outputs.device\n",
    "            n_agents = outputs.shape[0]\n",
    "            max_out = lambda i: torch.max(outputs[i, ...], axis=-1)\n",
    "            _, deciding_ags = torch.max(\n",
    "                torch.stack([max_out(i)[0] for i in range(n_agents)]), axis=0\n",
    "            )\n",
    "            mask_1 = deciding_ags.unsqueeze(0).unsqueeze(-1).expand_as(outputs)\n",
    "            mask_2 = torch.einsum(\n",
    "                \"b, bcx -> bcx\",\n",
    "                torch.arange(n_agents).to(device),\n",
    "                torch.ones_like(outputs),\n",
    "            )\n",
    "            mask = mask_1 == mask_2\n",
    "\n",
    "            return (outputs * mask).sum(0), deciding_ags\n",
    "\n",
    "        elif agent_decision == \"both\":\n",
    "            return outputs, None\n",
    "\n",
    "        elif \"sum\" in agent_decision:\n",
    "            return outputs.sum(0), None\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "\n",
    "from community.common.decision import get_decision\n",
    "\n",
    "\n",
    "def check_grad(model, task_id=\"0\"):\n",
    "    for n, p in model.named_parameters():\n",
    "        if \"k_params\" in n or \"all_scores\" in n:\n",
    "            if task_id in n:\n",
    "                return check_ind_grad(n, p)\n",
    "        else:\n",
    "            check_ind_grad(n, p)\n",
    "\n",
    "\n",
    "def check_ind_grad(n, p):\n",
    "    if p.grad is not None:\n",
    "        if (p.grad == 0).all():\n",
    "            \"\"\"\"\"\"\n",
    "            print(f\"{n}, Zero Grad\")\n",
    "        # else : print(f'{n} : {p.grad}')\n",
    "    elif p.requires_grad:\n",
    "        \"\"\"\"\"\"\n",
    "        print(f\"{n}, None Grad\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[] []\n",
      "[] []\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                [-1, 1, 25]          62,500\n",
      "================================================================\n",
      "Total params: 62,500\n",
      "Trainable params: 62,500\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.24\n",
      "Estimated Total Size (MB): 0.25\n",
      "----------------------------------------------------------------\n",
      "torch.Size([256, 6, 2, 50, 50])\n"
     ]
    }
   ],
   "source": [
    "task = \"0\"\n",
    "decision_params = (\"last\", \"0\")  # Change to '0', '1' or 'loss'\n",
    "\n",
    "dims = [\n",
    "    data_config[\"input_size\"] ** 2,\n",
    "    20,\n",
    "    n_classes if not (task == \"equal_count\" or task == \"nonzero\") else 2,\n",
    "]\n",
    "n_layers = 1\n",
    "sparsity = 0.0  # 1* 1/dims[1]**2\n",
    "\n",
    "use_conv = False\n",
    "binary_connections = False\n",
    "use_bottleneck = False\n",
    "\n",
    "dual_readout = [False, False]\n",
    "common_readout = False\n",
    "\n",
    "linear_ags = True\n",
    "\n",
    "dual_readout[0] = (task in [\"none\", \"both\"] and decision_params[1] == \"0\") or (\n",
    "    task == decision_params[1] == \"both\"\n",
    ")\n",
    "if task == \"both\" and common_readout:\n",
    "    dual_readout[1] = True\n",
    "    dual_readout[0] = False\n",
    "\n",
    "community = Ensemble(\n",
    "    dims,\n",
    "    sparsity,\n",
    "    use_conv,\n",
    "    binary_connections,\n",
    "    use_bottleneck,\n",
    "    dual_readout,\n",
    "    common_readout,\n",
    "    n_layers=1,\n",
    "    linear_ag=linear_ags,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(community.parameters(), lr=1e-3)\n",
    "\n",
    "gamma = 0.95  # ** (1/len(loaders[0]))\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma, verbose=False)\n",
    "# if use_conv :\n",
    "# summary(community.agents[0].conv, (1, data_config['input_size'], data_config['input_size']) if use_conv else (1, data_config['input_size']**2))\n",
    "\n",
    "summary(\n",
    "    community.agents[0],\n",
    "    (1, data_config[\"input_size\"], data_config[\"input_size\"])\n",
    "    if use_conv\n",
    "    else (1, data_config[\"input_size\"] ** 2),\n",
    ")\n",
    "\n",
    "# plot_confusion_mat(community)\n",
    "data, target = get_data(flatten=not use_conv, device=device)\n",
    "# print(community)\n",
    "# print(data.shape)\n",
    "\n",
    "out, states, fconns = community(data)\n",
    "if binary_connections:\n",
    "    print(fconns[-1].unique(return_counts=True))\n",
    "print(out.shape)\n",
    "# symbol_count(target).unique(return_counts=True), (symbol_count(target) == target[:, 0]).unique(return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([torch.tensor([0]) for _ in range(10)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ensemble(\n",
       "  (connections): ModuleList(\n",
       "    (0): Connection(in_features=20, out_features=20, bias=False)\n",
       "    (1): Connection(in_features=20, out_features=20, bias=False)\n",
       "  )\n",
       "  (agents): ModuleList(\n",
       "    (0): Agent(\n",
       "      (cell): RNN(2500, 20, bias=False)\n",
       "      (dropout): Dropout(p=1, inplace=False)\n",
       "      (readout): ModuleList(\n",
       "        (0): Linear(in_features=20, out_features=25, bias=False)\n",
       "        (1): Linear(in_features=20, out_features=25, bias=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Agent(\n",
       "      (cell): RNN(2500, 20, bias=False)\n",
       "      (dropout): Dropout(p=1, inplace=False)\n",
       "      (readout): ModuleList(\n",
       "        (0): Linear(in_features=20, out_features=25, bias=False)\n",
       "        (1): Linear(in_features=20, out_features=25, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "community\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = (\n",
    "    f'saves/network_{n_classes}{\"_non\"*(1 - data_config[\"static\"])}_static_p={sparsity}'\n",
    ")\n",
    "# torch.save(community.state_dict(), file_path)\n",
    "# community.load_state_dict(torch.load(file_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/home/gb21/Code/ANNs/community-of-agents/notebooks/Funcspec/Symbols.ipynb Cell 24\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bblossom.ee.ic.ac.uk/home/gb21/Code/ANNs/community-of-agents/notebooks/Funcspec/Symbols.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Stop exec before training\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bblossom.ee.ic.ac.uk/home/gb21/Code/ANNs/community-of-agents/notebooks/Funcspec/Symbols.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Stop exec before training\n",
    "raise StopIteration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from community.funcspec.single_model_loop import (\n",
    "    init_and_train,\n",
    "    train_and_compute_metrics,\n",
    "    train_community,\n",
    ")\n",
    "from community.utils.configs import get_training_dict\n",
    "import yaml\n",
    "from yaml.loader import SafeLoader\n",
    "\n",
    "with open(\"../../latest_config.yml\", \"r\") as config_file:\n",
    "    config = yaml.load(config_file, SafeLoader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"task\"] = \"both\"\n",
    "config[\"training\"][\"decision_params\"] = (\"last\", \"both\")\n",
    "\n",
    "config[\"model_params\"][\"common_readout\"] = False\n",
    "\n",
    "if config[\"task\"] == \"both\":\n",
    "    if config[\"model_params\"][\"common_readout\"]:\n",
    "        config[\"model_params\"][\"common_dual_readout\"] = True\n",
    "        config[\"model_params\"][\"agents_params\"][\"ag_dual_readout\"] = False\n",
    "    else:\n",
    "        config[\"model_params\"][\"common_dual_readout\"] = False\n",
    "        config[\"model_params\"][\"agents_params\"][\"ag_dual_readout\"] = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84677c9c9d0d4c17ba7191759bf5742f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch::   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'train_losses': array([3.21885443, 3.21785522, 3.21474075, ..., 0.66619003, 0.67224205,\n",
       "        0.68592483]),\n",
       " 'train_accs': array([[[0.03125   , 0.03515625],\n",
       "         [0.02734375, 0.06640625]],\n",
       " \n",
       "        [[0.0625    , 0.02734375],\n",
       "         [0.04296875, 0.046875  ]],\n",
       " \n",
       "        [[0.0390625 , 0.046875  ],\n",
       "         [0.05859375, 0.0625    ]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0.9140625 , 0.79296875],\n",
       "         [0.8828125 , 0.80078125]],\n",
       " \n",
       "        [[0.86328125, 0.8046875 ],\n",
       "         [0.8203125 , 0.79296875]],\n",
       " \n",
       "        [[0.87890625, 0.8125    ],\n",
       "         [0.83203125, 0.79296875]]], dtype=float32),\n",
       " 'test_losses': array([2.65767193, 2.39190769, 2.18305802, 2.00283885, 1.83691609,\n",
       "        1.68881643, 1.54351592, 1.42158163, 1.32628167, 1.25249135,\n",
       "        1.18526554, 1.13106275, 1.07964325, 1.02988875, 0.99989563,\n",
       "        0.97037995, 0.94495469, 0.91190583, 0.89239621, 0.85541445,\n",
       "        0.83586788, 0.81724691, 0.79482633, 0.78686613, 0.77442741,\n",
       "        0.74007213, 0.74612993, 0.71644688, 0.69948268, 0.67983538]),\n",
       " 'test_accs': array([[[0.12099359, 0.14182693],\n",
       "         [0.11738782, 0.13391426]],\n",
       " \n",
       "        [[0.19561298, 0.24969952],\n",
       "         [0.19160657, 0.26702723]],\n",
       " \n",
       "        [[0.26913062, 0.32151443],\n",
       "         [0.29156652, 0.31820914]],\n",
       " \n",
       "        [[0.35446715, 0.37199518],\n",
       "         [0.3716947 , 0.3951322 ]],\n",
       " \n",
       "        [[0.44320914, 0.44941908],\n",
       "         [0.44521233, 0.45903444]],\n",
       " \n",
       "        [[0.44961938, 0.47125402],\n",
       "         [0.50020033, 0.47956732]],\n",
       " \n",
       "        [[0.5463742 , 0.5016026 ],\n",
       "         [0.49879807, 0.49489182]],\n",
       " \n",
       "        [[0.6140825 , 0.55709136],\n",
       "         [0.5988582 , 0.51692706]],\n",
       " \n",
       "        [[0.6761819 , 0.6026643 ],\n",
       "         [0.6238982 , 0.5144231 ]],\n",
       " \n",
       "        [[0.67578125, 0.5988582 ],\n",
       "         [0.6376202 , 0.559996  ]],\n",
       " \n",
       "        [[0.69290864, 0.63271236],\n",
       "         [0.63311297, 0.5907452 ]],\n",
       " \n",
       "        [[0.7223558 , 0.62640226],\n",
       "         [0.6676683 , 0.5980569 ]],\n",
       " \n",
       "        [[0.73447514, 0.6608574 ],\n",
       "         [0.68008816, 0.60236377]],\n",
       " \n",
       "        [[0.7566106 , 0.6509415 ],\n",
       "         [0.70122194, 0.63752   ]],\n",
       " \n",
       "        [[0.75240386, 0.6780849 ],\n",
       "         [0.71965146, 0.6504407 ]],\n",
       " \n",
       "        [[0.7611178 , 0.7011218 ],\n",
       "         [0.74138623, 0.5900441 ]],\n",
       " \n",
       "        [[0.7645232 , 0.6666667 ],\n",
       "         [0.72475964, 0.69340944]],\n",
       " \n",
       "        [[0.78926283, 0.7155449 ],\n",
       "         [0.73667866, 0.6702724 ]],\n",
       " \n",
       "        [[0.8107973 , 0.7333734 ],\n",
       "         [0.75550884, 0.73127   ]],\n",
       " \n",
       "        [[0.82061297, 0.7358774 ],\n",
       "         [0.7901643 , 0.6875    ]],\n",
       " \n",
       "        [[0.82051283, 0.744391  ],\n",
       "         [0.7911659 , 0.7240585 ]],\n",
       " \n",
       "        [[0.8420473 , 0.74248797],\n",
       "         [0.76542467, 0.7530048 ]],\n",
       " \n",
       "        [[0.85747194, 0.77373797],\n",
       "         [0.79917866, 0.7772436 ]],\n",
       " \n",
       "        [[0.8292268 , 0.75741184],\n",
       "         [0.7916667 , 0.71814907]],\n",
       " \n",
       "        [[0.864984  , 0.7530048 ],\n",
       "         [0.7759415 , 0.7538061 ]],\n",
       " \n",
       "        [[0.85967547, 0.77483976],\n",
       "         [0.80959535, 0.76332134]],\n",
       " \n",
       "        [[0.8034856 , 0.77534056],\n",
       "         [0.8186098 , 0.7707332 ]],\n",
       " \n",
       "        [[0.87780446, 0.7692308 ],\n",
       "         [0.7927684 , 0.79046476]],\n",
       " \n",
       "        [[0.8913261 , 0.80558896],\n",
       "         [0.8078926 , 0.77844554]],\n",
       " \n",
       "        [[0.86949116, 0.7823518 ],\n",
       "         [0.8432492 , 0.7983774 ]]], dtype=float32),\n",
       " 'deciding_agents': array([], shape=(30, 0), dtype=float64),\n",
       " 'best_state': OrderedDict([('connections.0.weight',\n",
       "               tensor([[-0.0328,  0.0028,  0.0188, -0.0001, -0.0169, -0.0021, -0.0381, -0.0231,\n",
       "                         0.0103,  0.0107, -0.0047,  0.0489, -0.0126,  0.0015,  0.0184,  0.0245,\n",
       "                        -0.0094, -0.0199,  0.0004, -0.0082],\n",
       "                       [ 0.0009, -0.0058,  0.0047,  0.0031, -0.0002,  0.0123,  0.0164,  0.0281,\n",
       "                         0.0015,  0.0296,  0.0009, -0.0007,  0.0482,  0.0596, -0.0152, -0.0258,\n",
       "                        -0.0113, -0.0103, -0.0294,  0.0292],\n",
       "                       [ 0.0236, -0.0235, -0.0213, -0.0143,  0.0090,  0.0106, -0.0190, -0.0064,\n",
       "                        -0.0199, -0.0003, -0.0054,  0.0086, -0.0376, -0.0086, -0.0217,  0.0175,\n",
       "                        -0.0024,  0.0264, -0.0720, -0.0430],\n",
       "                       [-0.0398, -0.0500, -0.0020, -0.0240, -0.0128, -0.0071, -0.0019,  0.0159,\n",
       "                        -0.0048, -0.0050, -0.0050, -0.0255,  0.0481, -0.0308,  0.0059, -0.0221,\n",
       "                         0.0010, -0.0153,  0.0148,  0.0099],\n",
       "                       [-0.0266,  0.0110,  0.0171, -0.0151, -0.0105, -0.0123, -0.0446, -0.0292,\n",
       "                         0.0084, -0.0099, -0.0038, -0.0056,  0.0031, -0.0159, -0.0007,  0.0019,\n",
       "                         0.0409, -0.0127,  0.0500, -0.0024],\n",
       "                       [-0.0180, -0.0012, -0.0034,  0.0027,  0.0487,  0.0075,  0.0058, -0.0495,\n",
       "                         0.0356,  0.0153, -0.0114, -0.0002, -0.0481, -0.0340, -0.0128,  0.0040,\n",
       "                         0.0226,  0.0095,  0.0307, -0.0046],\n",
       "                       [ 0.0057,  0.0173, -0.0454,  0.0436, -0.0151, -0.0133, -0.0179,  0.0350,\n",
       "                         0.0221,  0.0047,  0.0132, -0.0194,  0.0224, -0.0185,  0.0207,  0.0130,\n",
       "                         0.0039, -0.0054,  0.0155, -0.0075],\n",
       "                       [-0.0314,  0.0151, -0.0271,  0.0221, -0.0045,  0.0297,  0.0186, -0.0252,\n",
       "                         0.0109, -0.0101,  0.0029,  0.0160,  0.0230, -0.0341,  0.0314, -0.0027,\n",
       "                         0.0129,  0.0019, -0.0031,  0.0250],\n",
       "                       [-0.0313,  0.0238,  0.0376,  0.0409, -0.0070,  0.0382, -0.0194, -0.0042,\n",
       "                         0.0351, -0.0112, -0.0633, -0.0057,  0.0155,  0.0266,  0.0132,  0.0218,\n",
       "                         0.0099, -0.0095,  0.0443, -0.0104],\n",
       "                       [-0.0115,  0.0041, -0.0301,  0.0375, -0.0482, -0.0133, -0.0029, -0.0504,\n",
       "                         0.0062, -0.0026,  0.0303,  0.0320, -0.0146,  0.0267, -0.0114, -0.0270,\n",
       "                        -0.0046, -0.0089,  0.0192, -0.0120],\n",
       "                       [-0.0240, -0.0264, -0.0060,  0.0263,  0.0001, -0.0057,  0.0046,  0.0211,\n",
       "                         0.0048,  0.0040,  0.0346, -0.0110,  0.0393, -0.0120,  0.0137, -0.0104,\n",
       "                         0.0019, -0.0305,  0.0006,  0.0091],\n",
       "                       [-0.0031, -0.0085, -0.0316, -0.0322,  0.0030, -0.0102, -0.0111, -0.0138,\n",
       "                         0.0191, -0.0013, -0.0273, -0.0175,  0.0069,  0.0098,  0.0231,  0.0071,\n",
       "                        -0.0277, -0.0312, -0.0169, -0.0249],\n",
       "                       [-0.0284, -0.0141,  0.0090, -0.0066,  0.0346,  0.0353, -0.0060, -0.0227,\n",
       "                         0.0232, -0.0079, -0.0248, -0.0036,  0.0196,  0.0096, -0.0237,  0.0065,\n",
       "                         0.0117, -0.0115,  0.0204,  0.0291],\n",
       "                       [-0.0061,  0.0199, -0.0171, -0.0393, -0.0248, -0.0055, -0.0043,  0.0082,\n",
       "                         0.0086, -0.0040, -0.0380,  0.0550, -0.0058,  0.0079, -0.0054,  0.0173,\n",
       "                         0.0082, -0.0237, -0.0172,  0.0235],\n",
       "                       [ 0.0294,  0.0205,  0.0069,  0.0005,  0.0182,  0.0141, -0.0044, -0.0191,\n",
       "                         0.0213, -0.0244,  0.0384, -0.0132, -0.0345,  0.0187, -0.0040,  0.0538,\n",
       "                         0.0252,  0.0544,  0.0382, -0.0146],\n",
       "                       [-0.0430, -0.0198,  0.0429, -0.0114,  0.0371, -0.0443,  0.0044, -0.0040,\n",
       "                         0.0171, -0.0296, -0.0287,  0.0189, -0.0039,  0.0211, -0.0101,  0.0234,\n",
       "                        -0.0273, -0.0327, -0.0710,  0.0073],\n",
       "                       [-0.0298, -0.0248, -0.0088, -0.0038, -0.0177, -0.0330,  0.0104,  0.0313,\n",
       "                        -0.0081, -0.0262,  0.0021,  0.0456, -0.0045,  0.0188, -0.0087, -0.0012,\n",
       "                        -0.0145,  0.0208, -0.0103,  0.0072],\n",
       "                       [-0.0793,  0.0075, -0.0378, -0.0154,  0.0135,  0.0127,  0.0233, -0.0034,\n",
       "                        -0.0065,  0.0213,  0.0007,  0.0484, -0.0353, -0.0556,  0.0303,  0.0237,\n",
       "                        -0.0078, -0.0048, -0.0107,  0.0042],\n",
       "                       [ 0.0245, -0.0029,  0.0276,  0.0087, -0.0267, -0.0175, -0.0021,  0.0219,\n",
       "                         0.0249, -0.0029,  0.0256,  0.0417,  0.0295, -0.0376,  0.0190,  0.0199,\n",
       "                        -0.0208,  0.0394,  0.0258,  0.0176],\n",
       "                       [ 0.0051, -0.0359, -0.0045, -0.0457,  0.0176,  0.0205, -0.0026,  0.0020,\n",
       "                         0.0228,  0.0110, -0.0031,  0.0036,  0.0190,  0.0410, -0.0290,  0.0165,\n",
       "                         0.0099, -0.0091,  0.0194,  0.0377]], device='cuda:0')),\n",
       "              ('connections.0.w_mask',\n",
       "               tensor([[False, False, False, False, False, False, False, False, False, False,\n",
       "                        False, False, False, False, False, False, False, False, False, False],\n",
       "                       [False, False, False, False, False, False, False, False, False, False,\n",
       "                        False, False, False, False, False, False, False, False, False, False],\n",
       "                       [False, False, False, False, False, False, False, False, False, False,\n",
       "                        False, False, False, False, False, False, False, False, False, False],\n",
       "                       [False, False, False, False, False, False, False, False, False, False,\n",
       "                        False, False, False, False, False, False, False, False, False, False],\n",
       "                       [False, False, False, False, False, False, False, False, False, False,\n",
       "                        False, False, False, False, False, False, False, False, False, False],\n",
       "                       [False, False, False, False, False, False, False, False, False, False,\n",
       "                        False, False, False, False, False, False, False, False, False, False],\n",
       "                       [False, False, False, False, False, False, False, False, False, False,\n",
       "                        False, False, False, False, False, False, False, False, False, False],\n",
       "                       [False, False, False, False, False, False, False, False, False, False,\n",
       "                        False, False, False, False, False, False, False, False, False, False],\n",
       "                       [False, False, False, False, False, False, False, False, False, False,\n",
       "                        False, False, False, False, False, False, False, False, False, False],\n",
       "                       [False, False, False, False, False, False, False, False, False, False,\n",
       "                        False, False, False, False, False, False, False, False, False, False],\n",
       "                       [False, False, False, False, False, False, False, False, False, False,\n",
       "                        False, False, False, False, False, False, False, False, False, False],\n",
       "                       [False, False, False, False, False, False, False, False, False, False,\n",
       "                        False, False, False, False, False, False, False, False, False, False],\n",
       "                       [False, False, False, False, False, False, False, False, False, False,\n",
       "                        False, False, False, False, False, False, False, False, False, False],\n",
       "                       [False, False, False, False, False, False, False, False, False, False,\n",
       "                        False, False, False, False, False, False, False, False, False, False],\n",
       "                       [False, False, False, False, False, False, False, False, False, False,\n",
       "                        False, False, False, False, False, False, False, False, False, False],\n",
       "                       [False, False, False, False, False, False, False, False, False, False,\n",
       "                        False, False, False, False, False, False, False, False, False, False],\n",
       "                       [False, False, False, False, False, False, False, False, False, False,\n",
       "                        False, False, False, False, False, False, False, False, False, False],\n",
       "                       [False, False, False, False, False, False, False, False, False, False,\n",
       "                        False, False, False, False, False, False, False, False, False, False],\n",
       "                       [False, False, False, False, False, False, False, False, False, False,\n",
       "                        False, False, False, False, False, False, False, False, False, False],\n",
       "                       [False, False, False, False, False, False, False, False, False, False,\n",
       "                        False, False, False, False, False, False, False, False, False, False]],\n",
       "                      device='cuda:0')),\n",
       "              ('connections.1.weight',\n",
       "               tensor([[-5.0280e-02,  3.5650e-02, -1.5777e-02, -6.5261e-03, -4.0967e-02,\n",
       "                        -8.3467e-03,  3.2300e-02,  1.4294e-02, -3.5788e-05, -2.7822e-05,\n",
       "                         1.5086e-02,  1.2576e-02, -1.3055e-02,  1.2377e-02,  7.1102e-03,\n",
       "                         2.5770e-03, -4.8022e-02,  5.1873e-03,  1.4035e-02, -1.2594e-02],\n",
       "                       [-2.6744e-02, -2.5894e-03,  7.1646e-03, -4.1080e-02, -3.2023e-02,\n",
       "                        -6.4999e-03,  4.4417e-03,  4.3145e-02,  1.3930e-03, -2.9221e-02,\n",
       "                        -1.1778e-02,  6.6299e-03,  3.0592e-02, -7.7278e-04,  1.8722e-02,\n",
       "                         3.1559e-02,  1.0359e-02,  2.6845e-02, -2.2893e-04,  1.6937e-02],\n",
       "                       [ 1.1618e-03, -2.6816e-02,  1.6519e-02,  9.9218e-03,  8.5896e-03,\n",
       "                        -1.5453e-03,  1.6809e-02, -2.6313e-02,  2.7426e-02, -2.2307e-02,\n",
       "                        -3.6439e-02, -1.7007e-02,  4.1639e-02, -5.0515e-03,  4.7648e-04,\n",
       "                        -3.3511e-02, -6.6510e-03, -1.6404e-02, -1.4925e-02,  1.0978e-02],\n",
       "                       [ 3.6330e-02, -3.1820e-02, -5.4650e-02, -1.1437e-02,  1.9721e-02,\n",
       "                         1.7800e-02, -2.8149e-02, -1.3695e-02, -1.9684e-03, -2.9390e-02,\n",
       "                         5.8264e-02, -4.7611e-03, -3.5073e-02, -1.6214e-02,  5.3860e-02,\n",
       "                         4.9530e-02, -4.7282e-03, -3.3440e-02,  4.4578e-02,  2.1288e-02],\n",
       "                       [-1.8761e-02,  9.0128e-03, -3.8408e-02,  9.2925e-03, -2.0410e-02,\n",
       "                        -2.0430e-02, -6.9525e-03, -3.1553e-02, -3.7855e-02,  9.0288e-03,\n",
       "                        -7.5434e-04,  3.0877e-02, -3.2173e-02,  7.3124e-03,  3.5194e-02,\n",
       "                        -1.0547e-02, -1.2544e-02, -3.2829e-02,  2.6435e-02, -9.4977e-03],\n",
       "                       [ 2.4919e-02,  5.0604e-02,  1.4398e-02, -1.1361e-02,  1.0766e-02,\n",
       "                        -1.5931e-02,  1.2594e-02,  1.5184e-02, -1.1633e-02,  4.4654e-03,\n",
       "                         8.3906e-03,  1.9331e-02, -1.9261e-02, -3.1797e-02, -2.7796e-02,\n",
       "                         2.5591e-03,  7.3737e-03, -3.7042e-02, -2.5442e-02,  8.4670e-03],\n",
       "                       [-2.4863e-02,  2.7724e-02, -1.7726e-02,  3.3719e-02,  3.7107e-03,\n",
       "                         4.4845e-02,  5.2231e-03, -3.4818e-02,  2.9661e-02,  3.0564e-02,\n",
       "                         2.9410e-02,  1.7810e-02,  2.0439e-02, -1.6316e-02,  2.0617e-03,\n",
       "                        -2.3309e-02, -1.4221e-03,  5.5267e-03, -1.5524e-03, -3.3785e-02],\n",
       "                       [-3.3737e-03, -3.0157e-02, -3.6282e-03,  3.5047e-02, -4.1922e-02,\n",
       "                        -2.2186e-02,  2.2196e-02,  7.7905e-03,  1.1054e-02,  1.0133e-02,\n",
       "                        -1.2352e-02,  1.5275e-02,  7.0226e-03,  7.6304e-03, -2.5798e-02,\n",
       "                        -2.8640e-02,  2.7164e-02,  1.3974e-02,  4.2227e-02, -3.3476e-02],\n",
       "                       [-6.9662e-03, -1.7431e-02,  2.8497e-02,  3.1530e-02,  1.6233e-02,\n",
       "                        -6.0926e-03,  3.5393e-02,  1.4342e-02, -4.5740e-03,  1.3634e-02,\n",
       "                        -2.1377e-03, -3.9252e-04, -1.8904e-03, -1.4279e-02,  3.0019e-02,\n",
       "                        -3.6117e-02,  2.5668e-02,  7.4633e-03,  1.2400e-02,  1.4145e-02],\n",
       "                       [ 1.8002e-02, -1.0827e-02,  2.2672e-02, -1.3583e-03, -3.3277e-02,\n",
       "                         1.3035e-02,  3.8467e-02, -2.5747e-02,  2.8100e-02, -1.7814e-02,\n",
       "                         3.0057e-02,  1.9395e-02, -1.3935e-02, -1.4419e-02,  6.0056e-03,\n",
       "                        -3.1864e-02, -9.7274e-03, -4.5032e-03,  4.6553e-02, -2.0269e-02],\n",
       "                       [-1.4927e-02,  6.0555e-03, -2.7702e-02, -2.5178e-02, -4.6997e-03,\n",
       "                        -8.8062e-03,  4.6121e-02, -9.2578e-03,  6.3684e-03,  4.2937e-02,\n",
       "                        -2.6497e-02, -4.7244e-02,  4.6187e-03, -2.6257e-02,  1.2696e-02,\n",
       "                        -5.2587e-02, -8.2836e-03, -1.3453e-02,  1.4131e-02,  1.6231e-02],\n",
       "                       [-5.0582e-02,  2.5933e-03, -4.3599e-02,  7.0596e-03, -1.6443e-02,\n",
       "                         1.7940e-02,  6.1891e-02, -1.2805e-02,  2.6550e-02, -2.4745e-02,\n",
       "                        -4.4315e-04,  1.0299e-02,  3.2888e-02, -2.0491e-02, -3.0608e-03,\n",
       "                         1.6993e-02,  1.7202e-02,  1.8972e-02, -6.7790e-03,  8.6407e-03],\n",
       "                       [-1.0162e-02, -2.3889e-03,  1.9306e-02, -1.2348e-03,  3.0342e-02,\n",
       "                         4.0968e-03, -2.1432e-02, -8.8327e-03, -1.9428e-02, -2.4942e-02,\n",
       "                        -2.0414e-02, -6.9147e-03,  3.5922e-02,  1.6855e-02, -7.1399e-03,\n",
       "                        -3.2877e-03,  2.7078e-03, -1.0321e-02,  1.8137e-02,  1.1032e-02],\n",
       "                       [ 4.5950e-02,  1.1450e-02,  1.4366e-02, -4.6448e-03, -2.5757e-02,\n",
       "                        -2.9249e-02,  1.4777e-02,  9.3649e-03,  1.8394e-02, -4.1014e-02,\n",
       "                         2.5593e-02,  2.4620e-02,  5.2497e-03,  1.9864e-02,  2.4557e-02,\n",
       "                        -1.6069e-02, -7.0520e-03, -4.7120e-02,  3.0675e-02, -7.6906e-04],\n",
       "                       [-9.5081e-03, -1.2081e-02, -1.1762e-02, -3.3753e-02,  4.6526e-03,\n",
       "                         1.6070e-02,  2.3447e-03,  3.3831e-02,  2.6857e-02,  9.8860e-03,\n",
       "                         2.8582e-02, -8.8333e-03,  1.9380e-02,  6.0234e-03, -3.1739e-02,\n",
       "                         6.0257e-02,  5.7579e-03,  2.4622e-02, -2.9761e-02, -3.2057e-03],\n",
       "                       [ 3.5211e-02, -1.5120e-02,  2.1543e-02, -2.6012e-02,  1.1936e-02,\n",
       "                        -2.0296e-03,  7.0906e-04, -1.0272e-02, -1.3543e-02, -1.3647e-02,\n",
       "                         1.9777e-02,  1.6877e-02,  1.7366e-02,  1.0106e-02, -5.7933e-03,\n",
       "                        -2.6284e-02,  5.2898e-03,  4.3568e-02, -5.4259e-02,  1.3673e-03],\n",
       "                       [ 2.3199e-02,  1.4698e-02, -1.7266e-02,  1.6722e-02,  3.8044e-04,\n",
       "                         7.5254e-03,  6.5056e-03, -4.1765e-03, -3.4095e-03, -7.3975e-04,\n",
       "                        -7.4961e-04,  1.6868e-02, -4.6286e-02,  4.4423e-03, -3.8169e-02,\n",
       "                         1.6074e-02, -2.5564e-02, -2.7028e-02, -5.8135e-03, -4.2366e-02],\n",
       "                       [-1.9714e-02, -6.0759e-02, -6.3963e-03, -2.7209e-03,  3.0373e-02,\n",
       "                        -1.5011e-02,  4.6046e-03, -1.6302e-02, -5.1549e-03, -1.4594e-02,\n",
       "                         1.3110e-02,  2.3095e-02, -3.0353e-02,  6.2468e-02, -2.7246e-02,\n",
       "                        -2.6072e-02,  1.2819e-02,  1.1274e-02,  5.8581e-03,  1.4389e-02],\n",
       "                       [ 6.9856e-02,  2.6566e-02,  2.3947e-02,  1.2189e-02,  1.7168e-02,\n",
       "                         1.7185e-02, -2.8819e-02, -1.8724e-02,  2.9933e-02,  2.1393e-04,\n",
       "                         3.3410e-02, -2.4923e-02,  1.6655e-02, -2.1643e-02, -6.5787e-02,\n",
       "                         4.0721e-04,  1.6402e-02,  9.3342e-03, -1.9352e-02,  2.7078e-02],\n",
       "                       [-2.4809e-02,  8.7699e-03,  2.0089e-02,  2.8839e-02, -5.5551e-03,\n",
       "                        -4.6528e-03, -1.4211e-02, -2.5830e-03, -2.8528e-03, -5.4873e-02,\n",
       "                        -5.2180e-03,  4.1228e-02, -2.0070e-02, -2.3010e-02,  1.6577e-02,\n",
       "                        -1.2597e-02,  1.4004e-02, -7.7599e-03,  1.3828e-02, -1.9747e-02]],\n",
       "                      device='cuda:0')),\n",
       "              ('connections.1.w_mask',\n",
       "               tensor([[False, False, False, False, False, False, False, False, False, False,\n",
       "                        False, False, False, False, False, False, False, False, False, False],\n",
       "                       [False, False, False, False, False, False, False, False, False, False,\n",
       "                        False, False, False, False, False, False, False, False, False, False],\n",
       "                       [False, False, False, False, False, False, False, False, False, False,\n",
       "                        False, False, False, False, False, False, False, False, False, False],\n",
       "                       [False, False, False, False, False, False, False, False, False, False,\n",
       "                        False, False, False, False, False, False, False, False, False, False],\n",
       "                       [False, False, False, False, False, False, False, False, False, False,\n",
       "                        False, False, False, False, False, False, False, False, False, False],\n",
       "                       [False, False, False, False, False, False, False, False, False, False,\n",
       "                        False, False, False, False, False, False, False, False, False, False],\n",
       "                       [False, False, False, False, False, False, False, False, False, False,\n",
       "                        False, False, False, False, False, False, False, False, False, False],\n",
       "                       [False, False, False, False, False, False, False, False, False, False,\n",
       "                        False, False, False, False, False, False, False, False, False, False],\n",
       "                       [False, False, False, False, False, False, False, False, False, False,\n",
       "                        False, False, False, False, False, False, False, False, False, False],\n",
       "                       [False, False, False, False, False, False, False, False, False, False,\n",
       "                        False, False, False, False, False, False, False, False, False, False],\n",
       "                       [False, False, False, False, False, False, False, False, False, False,\n",
       "                        False, False, False, False, False, False, False, False, False, False],\n",
       "                       [False, False, False, False, False, False, False, False, False, False,\n",
       "                        False, False, False, False, False, False, False, False, False, False],\n",
       "                       [False, False, False, False, False, False, False, False, False, False,\n",
       "                        False, False, False, False, False, False, False, False, False, False],\n",
       "                       [False, False, False, False, False, False, False, False, False, False,\n",
       "                        False, False, False, False, False, False, False, False, False, False],\n",
       "                       [False, False, False, False, False, False, False, False, False, False,\n",
       "                        False, False, False, False, False, False, False, False, False, False],\n",
       "                       [False, False, False, False, False, False, False, False, False, False,\n",
       "                        False, False, False, False, False, False, False, False, False, False],\n",
       "                       [False, False, False, False, False, False, False, False, False, False,\n",
       "                        False, False, False, False, False, False, False, False, False, False],\n",
       "                       [False, False, False, False, False, False, False, False, False, False,\n",
       "                        False, False, False, False, False, False, False, False, False, False],\n",
       "                       [False, False, False, False, False, False, False, False, False, False,\n",
       "                        False, False, False, False, False, False, False, False, False, False],\n",
       "                       [False, False, False, False, False, False, False, False, False, False,\n",
       "                        False, False, False, False, False, False, False, False, False, False]],\n",
       "                      device='cuda:0')),\n",
       "              ('agents.0.cell.weight_ih_l0',\n",
       "               tensor([[ 0.0165,  0.0173, -0.0582,  ..., -0.0613,  0.0170,  0.0203],\n",
       "                       [-0.1262, -0.1232, -0.0361,  ..., -0.0357, -0.1215, -0.1223],\n",
       "                       [-0.0200, -0.0155,  0.0566,  ...,  0.0494, -0.0148, -0.0219],\n",
       "                       ...,\n",
       "                       [ 0.0306,  0.0271, -0.0689,  ..., -0.0544,  0.0256,  0.0246],\n",
       "                       [-0.0246, -0.0320,  0.0959,  ...,  0.0732, -0.0294, -0.0295],\n",
       "                       [ 0.0064,  0.0023, -0.0311,  ..., -0.0419,  0.0077,  0.0122]],\n",
       "                      device='cuda:0')),\n",
       "              ('agents.0.cell.weight_hh_l0',\n",
       "               tensor([[ 1.1651e+00, -7.0496e-01, -2.0502e+00,  1.0245e+00,  3.4483e-01,\n",
       "                        -5.1330e-01,  1.1152e-02, -4.8585e-01, -7.5906e-02, -8.5550e-02,\n",
       "                        -3.6453e-01,  8.8951e-02, -4.7631e-03,  1.3691e-01,  3.2478e-01,\n",
       "                        -1.1427e+00, -1.5488e-01, -2.1160e-01,  4.4443e-01,  7.0063e-01],\n",
       "                       [-1.4760e-01,  1.1355e-01,  2.9781e-01, -3.0943e-02, -1.8812e+00,\n",
       "                         1.7692e-01, -9.7532e-01, -7.9806e-02, -9.9749e-01, -1.1368e+00,\n",
       "                         3.8973e-02, -9.7133e-01, -9.1585e-01, -7.5831e-01,  1.0222e-01,\n",
       "                        -3.0474e-01,  3.0819e+00,  2.5545e-03, -5.7138e-02, -1.1903e-01],\n",
       "                       [ 3.3804e-01,  1.0856e+00,  1.3042e+00, -5.9615e-01, -1.0307e+00,\n",
       "                         7.6088e-01,  1.7836e-01,  6.6766e-01,  6.7305e-02, -2.4418e-01,\n",
       "                         6.2651e-01,  1.2537e-01, -2.3694e-02,  3.3009e-01, -2.8047e-01,\n",
       "                         1.0539e+00,  6.7443e-01,  4.1663e-01, -7.1103e-01, -2.0779e-01],\n",
       "                       [-6.5524e-01, -1.0221e+00,  8.8957e-03,  1.6268e+00,  8.1947e-01,\n",
       "                        -4.0009e-01,  2.8402e-01, -5.6002e-01,  2.2210e-01,  4.4959e-01,\n",
       "                        -6.1429e-01,  4.6463e-01,  2.0331e-01,  5.5454e-01, -4.9998e-01,\n",
       "                         7.4105e-01, -6.6305e-01, -6.4650e-01,  7.0881e-01,  1.0429e+00],\n",
       "                       [ 1.2421e-01,  1.2417e-01, -1.3071e-01,  1.8318e-01,  2.5707e-01,\n",
       "                        -2.0697e-01,  1.0591e+00,  2.0759e-01,  9.6008e-01,  1.5183e+00,\n",
       "                         8.5331e-02,  1.0392e+00,  1.0008e+00,  9.7371e-01, -1.3766e-02,\n",
       "                         5.0308e-01, -2.5620e+00,  1.0662e-01, -2.7473e-01,  3.5259e-01],\n",
       "                       [ 5.4483e-01, -1.0851e-01, -4.8834e-01,  4.8441e-01,  1.1302e-01,\n",
       "                         1.3446e+00,  2.5670e-01,  7.6756e-01,  1.1808e-01,  1.6188e-01,\n",
       "                         1.4657e+00,  3.1539e-01,  2.2999e-01,  5.2540e-01, -5.3075e-01,\n",
       "                        -2.2162e-01, -1.0375e-01,  5.6204e-01, -1.8783e+00, -3.1250e-01],\n",
       "                       [ 1.2919e-01,  8.0973e-01, -1.4256e-01,  1.5045e-02, -7.5611e-01,\n",
       "                        -8.7826e-02,  1.4480e+00, -3.1537e-02, -2.7950e-01, -5.9895e-01,\n",
       "                        -8.4523e-02,  1.3137e+00,  1.4091e+00,  1.1688e+00, -1.4356e-01,\n",
       "                         1.6964e-01,  6.4648e-01, -4.6530e-02, -4.3064e-01,  2.7567e-04],\n",
       "                       [ 1.3916e+00, -2.4086e-01, -6.8853e-01,  4.2759e-01, -9.6467e-03,\n",
       "                        -1.9982e-01, -1.0655e-03,  7.5142e-01, -3.9010e-02, -1.1970e-01,\n",
       "                         2.2679e-01, -2.5513e-01, -6.1400e-01, -4.8857e-02, -2.7032e-01,\n",
       "                        -6.3916e-01,  9.1341e-02,  1.9086e+00,  2.9813e-02,  3.9534e-01],\n",
       "                       [ 4.8049e-02,  7.0070e-01, -1.3100e-01, -1.0476e-01, -6.4404e-01,\n",
       "                         1.4015e-01,  2.0781e+00, -8.0684e-02,  9.5644e-01, -3.0586e-01,\n",
       "                        -1.5384e-02,  1.0415e+00,  1.1973e+00,  9.4458e-01, -2.3170e-02,\n",
       "                         3.7564e-01,  5.1169e-01, -1.1841e-01,  5.8637e-02, -1.3819e-01],\n",
       "                       [ 1.6461e-01,  1.9057e-01, -1.7881e-01,  7.4501e-02, -1.8602e-01,\n",
       "                         3.8638e-02,  1.2981e+00, -2.0168e-01,  2.4040e+00,  8.2065e-01,\n",
       "                         7.0470e-02,  7.8648e-01,  7.4271e-01,  6.7137e-01, -1.4867e-01,\n",
       "                         2.3933e-01, -3.8234e-02, -2.6509e-02, -3.4996e-01, -3.9439e-02],\n",
       "                       [ 9.6878e-01, -1.4542e-01, -6.6082e-01,  2.4989e-01,  1.4532e-01,\n",
       "                        -3.3376e-01, -3.5438e-01,  1.5247e+00, -2.4290e-03,  9.2264e-02,\n",
       "                         5.7015e-01, -1.7589e-01, -2.0680e-01, -1.4888e-02, -1.5733e-01,\n",
       "                        -6.0280e-01, -7.1974e-02,  1.7132e+00,  4.0892e-02,  5.4362e-01],\n",
       "                       [ 9.2282e-02,  1.0163e+00, -3.7156e-03, -6.3171e-02, -7.9369e-01,\n",
       "                        -2.6509e-02, -2.9516e-01, -2.2817e-02, -4.6844e-01, -6.0383e-01,\n",
       "                         1.1231e-01,  1.4578e+00, -1.2505e-01,  1.6344e+00, -1.0117e-01,\n",
       "                        -5.8118e-02,  6.9939e-01, -1.5469e-01, -7.0672e-01, -3.4602e-03],\n",
       "                       [ 6.5425e-02,  7.4940e-01, -6.7426e-02, -4.3779e-02, -6.5805e-01,\n",
       "                         9.3700e-03, -4.1228e-01, -3.4757e-02, -6.2608e-01, -7.1107e-01,\n",
       "                         1.5000e-01,  1.6349e+00,  1.5099e+00,  1.4836e+00, -9.6886e-02,\n",
       "                         1.8793e-01,  6.4323e-01, -1.7174e-01, -7.5259e-01, -8.2812e-03],\n",
       "                       [ 1.2411e-01,  1.3543e+00, -8.9850e-03, -2.8322e-01, -1.3356e+00,\n",
       "                        -8.8721e-02, -4.1944e-01, -4.5127e-02, -6.0808e-01, -7.1721e-01,\n",
       "                         6.7645e-02,  1.8978e-01, -2.2942e-01,  1.2138e+00, -2.8020e-01,\n",
       "                        -2.9666e-01,  7.5898e-01, -1.7288e-01, -3.8322e-01, -1.1474e-01],\n",
       "                       [-3.8444e-01, -7.9848e-01, -1.8183e+00,  2.7292e-01,  4.4461e-01,\n",
       "                        -3.6933e-01,  7.0230e-01,  3.3979e-01,  4.3966e-01,  4.3171e-01,\n",
       "                         5.8227e-01,  9.6727e-01,  5.5154e-01,  1.1070e+00,  7.0013e-01,\n",
       "                        -8.3098e-01, -3.4650e-01,  9.2708e-01,  2.1633e-01, -1.1588e+00],\n",
       "                       [ 5.3644e-01, -2.1478e-01, -9.7066e-02, -1.1308e+00,  2.0804e-01,\n",
       "                         1.0317e+00,  5.7224e-01,  8.0113e-01,  3.5594e-01,  1.9685e-01,\n",
       "                         8.7903e-01,  4.0561e-01,  2.7158e-01,  6.3688e-01,  8.9299e-01,\n",
       "                         1.2611e+00, -1.8360e-01,  7.5417e-01, -1.1419e+00, -8.0772e-01],\n",
       "                       [-1.1135e-01, -2.2341e-01,  3.1199e-01, -6.2033e-02,  1.0348e-01,\n",
       "                         1.1995e-01, -8.4589e-01,  6.6537e-02, -1.9869e+00, -2.0800e+00,\n",
       "                        -5.6249e-02, -8.3839e-01, -7.4122e-01, -7.9467e-01, -9.3134e-03,\n",
       "                        -3.4513e-01,  7.0419e-01,  3.6180e-02,  2.2793e-01, -4.7429e-02],\n",
       "                       [ 2.4813e+00, -2.7372e-01, -1.3800e+00,  4.7366e-01,  1.2664e-01,\n",
       "                        -4.2402e-01, -1.1814e-01, -6.4930e-02, -1.0747e-01, -1.4332e-01,\n",
       "                        -9.5645e-02, -5.0092e-01, -1.8088e-01, -4.4584e-01,  1.0809e-01,\n",
       "                        -4.7849e-01, -4.6958e-02,  7.3914e-01,  1.8596e-01,  4.6498e-01],\n",
       "                       [-9.4082e-01,  2.5643e-01,  6.7363e-01, -1.9101e-01, -1.5130e-01,\n",
       "                         2.9928e-01, -3.9362e-02, -1.8494e+00, -2.8617e-01, -3.1733e-01,\n",
       "                        -2.2014e+00,  2.4400e-02,  1.6419e-01, -3.6258e-01,  1.0851e-01,\n",
       "                         7.4080e-01,  1.2024e-01, -8.3731e-01,  6.3645e-01, -5.2128e-01],\n",
       "                       [-4.7419e-01, -5.9950e-02,  5.4895e-01, -4.5926e-01,  7.9826e-02,\n",
       "                        -1.5282e+00, -8.7788e-02, -7.4848e-01, -1.1859e-02,  1.9249e-02,\n",
       "                        -1.1796e+00,  7.6184e-03, -4.3436e-02,  5.4587e-02,  5.6666e-01,\n",
       "                         3.9989e-01, -1.2954e-02, -5.4587e-01,  1.2710e+00,  1.5130e+00]],\n",
       "                      device='cuda:0')),\n",
       "              ('agents.0.readout.0.weight',\n",
       "               tensor([[ 7.7141e-01, -4.1089e-01, -5.7175e-01, -1.2336e+00,  2.1719e-01,\n",
       "                         1.5510e+00, -1.4749e-01,  9.3062e-01, -8.3476e-02, -1.5792e-01,\n",
       "                         1.0398e+00,  1.3108e-01,  6.8873e-01,  3.6722e-01,  2.3353e-01,\n",
       "                         2.5219e+00, -1.5556e-01,  8.1438e-01, -1.5496e+00, -1.0380e+00],\n",
       "                       [ 5.2431e-01, -8.4586e-02, -4.6910e-01,  7.7916e-01,  1.3981e-01,\n",
       "                         2.8882e+00,  4.5802e-01,  8.5287e-01, -4.7413e-02,  7.4393e-02,\n",
       "                         9.4334e-01, -1.6574e-01,  2.2763e-01, -8.9995e-02, -1.1583e+00,\n",
       "                        -2.1656e+00, -4.7997e-02,  6.8419e-01, -1.8993e+00, -1.5695e+00],\n",
       "                       [ 7.5734e-01, -3.0939e-01, -5.1820e-01,  5.5498e-01,  2.2158e-02,\n",
       "                        -2.8564e+00, -2.8221e-01,  1.2750e+00, -1.8196e-01, -1.5504e-01,\n",
       "                         2.0221e+00,  1.5826e-01, -1.1581e-02,  5.0388e-01, -1.1871e-01,\n",
       "                        -1.6120e+00,  8.2682e-02,  1.0932e+00, -3.4637e+00,  7.5640e-01],\n",
       "                       [ 5.7636e-01, -1.0395e-01, -3.7163e-01,  7.3497e-01,  3.0259e-02,\n",
       "                        -1.4162e+00,  1.5677e-01,  1.0733e+00,  1.3302e-01,  9.0911e-02,\n",
       "                         4.4822e+00, -3.4382e-01, -2.0504e-01,  3.7838e-01, -8.1952e-01,\n",
       "                        -1.1031e+00,  2.4769e-02,  7.4014e-01,  2.1765e+00,  9.7609e-01],\n",
       "                       [ 8.6271e-01, -7.3168e-02, -5.5516e-01,  4.8077e-02, -2.5944e-02,\n",
       "                        -1.3539e+00, -4.8411e-02,  2.4994e+00, -1.2261e-01, -4.3780e-02,\n",
       "                        -1.6354e-01, -1.5716e-01, -5.0165e-02, -3.5576e-02, -3.9636e-01,\n",
       "                        -8.6611e-01, -2.0312e-02,  1.3546e+00,  2.4237e+00, -4.6812e-02],\n",
       "                       [ 8.9146e-01, -1.3320e-01, -6.2842e-01,  4.2231e-01, -5.0329e-02,\n",
       "                        -4.9096e-01, -3.2299e-01,  1.7664e+00, -1.6755e-01, -1.2061e-01,\n",
       "                        -3.8880e+00,  4.5099e-01, -2.6684e-01,  7.3221e-01,  1.2137e+00,\n",
       "                        -8.0103e-01,  1.4843e-01,  1.3827e+00,  6.6265e-01,  1.0960e+00],\n",
       "                       [ 1.9547e+00,  2.3137e-01, -7.8593e-01,  6.0919e-01, -1.9297e-01,\n",
       "                        -3.7341e-01, -1.6657e-01, -3.4013e+00, -1.7821e-01, -1.6311e-01,\n",
       "                        -1.3748e+00,  3.5465e-01, -2.1334e-01,  3.6129e-01, -8.8451e-02,\n",
       "                        -1.1845e+00,  1.8002e-01,  3.1666e+00,  5.1467e-01,  7.3322e-01],\n",
       "                       [ 3.8055e+00,  4.6407e-01, -1.0385e+00,  2.3318e-01, -3.4676e-01,\n",
       "                        -3.8332e-01, -1.0304e-01, -1.7219e+00, -7.8324e-02, -1.0834e-02,\n",
       "                        -8.2046e-01,  2.1910e-01, -2.4205e-01,  3.4049e-01,  7.2657e-01,\n",
       "                        -1.6683e+00,  7.2158e-02, -2.5373e+00,  4.2695e-01,  7.7100e-01],\n",
       "                       [-1.6208e+00,  2.4193e-01, -2.5207e+00,  7.0288e-01, -2.7964e-01,\n",
       "                        -4.6214e-01, -1.3515e-01, -9.6103e-01, -1.2670e-01, -1.9247e-01,\n",
       "                        -6.1604e-01,  2.2604e-01, -1.8700e-01,  5.1906e-01,  1.9352e+00,\n",
       "                        -2.1807e+00,  3.2599e-01, -1.6516e+00,  3.6761e-01,  8.2506e-01],\n",
       "                       [-2.6338e+00,  3.0251e-01, -2.9015e+00,  1.1526e+00, -3.9307e-01,\n",
       "                        -1.2295e-01, -7.5339e-02, -3.9113e-01, -2.0479e-01, -2.0534e-01,\n",
       "                        -1.5487e-01, -1.1038e-01,  9.5424e-02, -1.2597e-01, -2.6023e+00,\n",
       "                        -2.4117e+00,  3.9425e-01, -6.9741e-01,  1.6483e-03,  4.7963e-01],\n",
       "                       [-8.2386e-01, -1.6762e-01,  1.7807e+00,  1.8460e+00, -4.0861e-02,\n",
       "                        -6.5599e-03,  6.2715e-02, -1.5415e-01, -1.5064e-01, -4.2472e-02,\n",
       "                         9.9779e-03, -1.2522e-01,  1.4746e-01, -6.7018e-01, -1.8009e+00,\n",
       "                        -4.3700e+00, -1.5353e-02, -3.9314e-01, -1.8343e-01,  3.8474e-01],\n",
       "                       [-5.8405e-01,  1.4967e-01,  2.5245e+00,  2.3829e+00, -4.5220e-02,\n",
       "                        -3.6445e-01, -5.9585e-03, -2.8068e-01, -6.9352e-02, -1.2432e-02,\n",
       "                        -1.7955e-01,  1.0391e-01,  3.2756e-03, -3.9364e-01, -2.2004e+00,\n",
       "                        -6.0128e-01,  5.2978e-02, -3.9805e-01,  5.1248e-02,  8.4397e-01],\n",
       "                       [-2.6349e-01,  2.3880e-01,  7.3029e-01,  3.0141e+00, -2.0855e-01,\n",
       "                        -3.6003e-01, -8.5122e-02, -7.3660e-02, -6.6327e-03, -2.0227e-01,\n",
       "                        -6.8779e-02,  1.8592e-01, -1.3560e-01, -8.8642e-02, -2.8449e+00,\n",
       "                         3.5696e+00,  7.8217e-02, -2.0116e-01, -8.0200e-02,  9.2164e-01],\n",
       "                       [-1.4050e-01, -1.6039e-01,  3.6808e-01, -3.7048e-01,  1.9357e-01,\n",
       "                        -3.7723e-01,  8.0886e-03, -1.2034e-02, -4.4005e-03,  8.0325e-02,\n",
       "                         4.9127e-02, -1.1350e-01,  1.2313e-01, -4.9094e-01, -3.8728e+00,\n",
       "                         2.5809e+00, -7.4470e-02, -1.0121e-01, -1.4570e-01,  1.2196e+00],\n",
       "                       [-6.6271e-02, -4.0990e-02,  2.4712e-01, -2.5089e+00,  2.8744e-01,\n",
       "                        -4.3286e-01,  1.4557e-01,  4.0269e-02,  2.2201e-02,  1.5141e-01,\n",
       "                         1.1792e-01, -7.8131e-02,  2.2640e-01, -6.5462e-01, -2.4493e+00,\n",
       "                         1.9228e+00, -2.6444e-01, -2.6863e-03, -1.9940e-01,  1.9332e+00],\n",
       "                       [-1.9505e-01, -7.6527e-02,  3.4471e-01, -1.8235e+00,  4.9086e-02,\n",
       "                        -5.9489e-01,  1.1113e-01, -3.2257e-02,  1.4756e-01,  1.7673e-01,\n",
       "                         7.6209e-02, -1.3229e-01,  7.7599e-02, -2.2275e-01,  3.2057e-01,\n",
       "                         1.5237e+00, -1.8603e-01, -8.2637e-02, -1.7688e-01,  3.3914e+00],\n",
       "                       [-1.4419e-01,  1.1546e-02,  3.4800e-01, -1.0483e+00,  2.1734e-02,\n",
       "                        -5.3069e-01,  2.9195e-02,  5.8410e-02,  2.5133e-01,  2.0070e-01,\n",
       "                         2.3299e-01, -1.0939e-01,  4.4768e-02,  2.8758e-02,  3.3836e+00,\n",
       "                         1.0442e+00, -1.0158e-01, -1.3450e-02, -3.8462e-01,  4.0774e+00],\n",
       "                       [-2.5152e-01,  1.1199e-01,  3.8339e-01, -8.5503e-01,  4.6219e-02,\n",
       "                        -1.6649e+00,  1.5422e-01, -7.3766e-02,  2.0135e-01,  1.1534e-01,\n",
       "                        -3.6696e-03, -4.3462e-02, -1.4048e-02, -3.5214e-02,  3.3751e+00,\n",
       "                         8.8940e-01, -1.2489e-01, -1.6654e-01, -2.1202e-01,  1.4504e+00],\n",
       "                       [-2.7652e-01,  1.3691e-01,  3.9662e-01, -6.0204e-01,  1.3703e-02,\n",
       "                        -2.9607e+00,  1.1129e-01, -1.5991e-01,  7.2583e-02,  1.8395e-01,\n",
       "                        -8.9162e-02,  6.8987e-02,  4.7678e-02, -1.5080e-01,  1.6754e+00,\n",
       "                         6.9729e-01, -2.3766e-02, -2.3760e-01, -2.8024e-02, -1.1508e+00],\n",
       "                       [-1.8573e-01,  2.3054e-01,  2.8300e-01, -3.9506e-01, -5.6459e-02,\n",
       "                        -3.0724e+00,  1.2749e-01, -1.2215e-01,  8.0165e-02,  6.2407e-02,\n",
       "                        -5.9386e-02,  8.5570e-02,  3.3304e-02, -1.8271e-01,  9.2661e-01,\n",
       "                         4.1245e-01, -1.5394e-01, -1.8684e-01, -1.0932e-01, -3.4942e+00],\n",
       "                       [-3.6759e-01,  9.7807e-02,  4.3993e-01, -5.5945e-01,  5.3249e-02,\n",
       "                        -1.0882e+00,  1.0564e-01, -3.0958e-01,  2.1348e-01,  7.2076e-02,\n",
       "                        -2.5848e-01,  3.9049e-02,  7.6406e-02, -5.5792e-02,  9.2845e-01,\n",
       "                         4.6015e-01, -4.4746e-02, -3.1649e-01,  1.2282e-01, -3.4237e+00],\n",
       "                       [-5.5596e-01, -4.5854e-02,  5.8902e-01, -6.7940e-01,  9.9560e-02,\n",
       "                         8.2487e-01,  9.4175e-02, -4.6276e-01,  1.2527e-01,  3.4979e-02,\n",
       "                        -4.6048e-01,  3.1946e-03,  4.1445e-02,  8.4647e-02,  9.4006e-01,\n",
       "                         5.6040e-01, -1.5574e-03, -5.2815e-01,  2.9069e-01, -2.3294e+00],\n",
       "                       [-5.6103e-01, -1.0770e-01,  5.7874e-01, -6.1042e-01,  4.4279e-02,\n",
       "                         2.6848e+00,  4.7113e-02, -4.9170e-01,  1.1511e-01,  5.6737e-02,\n",
       "                        -4.6648e-01, -1.2488e-02, -1.1526e-01,  6.4621e-02,  8.1012e-01,\n",
       "                         4.8375e-01, -8.5442e-02, -5.2670e-01,  3.0013e-01, -1.5873e+00],\n",
       "                       [-4.7043e-01, -9.1441e-02,  5.1995e-01, -5.2596e-01,  6.5786e-03,\n",
       "                         3.9121e+00,  6.2640e-03, -4.0062e-01,  1.7157e-01,  4.7569e-03,\n",
       "                        -4.2624e-01, -3.6915e-02, -1.3019e-01,  5.8999e-02,  6.5542e-01,\n",
       "                         3.7653e-01, -3.9774e-02, -5.1668e-01,  2.8955e-01, -1.3695e+00],\n",
       "                       [-4.4543e-01, -5.1175e-02,  5.1861e-01, -4.5772e-01,  1.1756e-01,\n",
       "                         4.3950e+00,  1.1373e-03, -3.8667e-01,  8.9026e-02,  7.0459e-02,\n",
       "                        -3.9358e-01, -6.7411e-02, -1.1200e-01,  8.4944e-02,  6.0807e-01,\n",
       "                         3.2012e-01, -2.8565e-03, -5.0360e-01,  2.5324e-01, -1.2603e+00]],\n",
       "                      device='cuda:0')),\n",
       "              ('agents.0.readout.1.weight',\n",
       "               tensor([[-7.8216e-01,  1.6163e+00,  8.5194e-01, -9.6460e-01, -1.3835e+00,\n",
       "                        -1.8499e-01,  1.1722e-01,  1.1453e-01, -1.0311e+00, -1.0928e+00,\n",
       "                        -1.7164e-01,  2.5553e+00,  6.1897e-01,  1.8060e+00, -6.0078e-01,\n",
       "                        -7.7058e-01,  1.1976e+00,  2.1782e-01,  3.4611e-01,  2.2351e-01],\n",
       "                       [ 3.4747e-01,  1.8048e+00,  5.8919e-01, -4.6317e-01, -1.1098e+00,\n",
       "                         2.0611e-02, -1.0725e+00,  1.3004e-02, -7.2100e-01, -7.7642e-01,\n",
       "                         1.0089e-01, -2.3517e+00, -1.0463e+00,  3.2197e+00, -2.4279e-01,\n",
       "                        -2.9828e-02,  8.8385e-01, -2.6451e-02, -9.1823e-03, -1.8539e-01],\n",
       "                       [-6.4230e-02,  3.1250e+00, -2.7466e-01, -4.1956e-01, -1.7489e+00,\n",
       "                         1.3148e-01, -8.2511e-01, -2.1694e-02, -8.5615e-01, -1.0109e+00,\n",
       "                         2.2902e-01, -1.5027e+00, -6.2642e-01, -3.1182e+00,  2.2789e-01,\n",
       "                         1.7707e-01,  1.1937e+00, -5.3438e-02, -1.8129e-01, -2.4917e-01],\n",
       "                       [ 3.0374e-02, -2.7175e+00, -2.1694e-01, -4.7077e-01, -3.3900e+00,\n",
       "                         2.0060e-01, -6.0861e-01, -3.3289e-02, -8.1639e-01, -9.8808e-01,\n",
       "                         2.1020e-02, -6.0797e-01, -3.7690e-01, -1.3976e+00, -2.1246e-02,\n",
       "                        -4.8532e-03,  1.8693e+00,  3.4142e-02, -2.6635e-02, -2.9162e-01],\n",
       "                       [ 6.5679e-02, -9.4161e-01, -1.4058e-01, -9.1392e-02,  2.5217e+00,\n",
       "                         3.1531e-01, -3.9057e-01, -1.9192e-02, -7.3259e-01, -1.2129e+00,\n",
       "                         1.1809e-01, -2.9035e-01, -2.5036e-01, -8.2011e-01,  2.3788e-01,\n",
       "                         4.2302e-02,  4.9280e+00,  3.2379e-02, -7.1790e-02, -1.4485e-01],\n",
       "                       [-4.0208e-02, -8.8394e-01, -3.6908e-02,  1.8650e-01,  1.4239e+00,\n",
       "                         4.9063e-01, -9.9042e-01,  4.5299e-02, -1.4965e+00, -2.6847e+00,\n",
       "                         2.5750e-01, -6.7564e-01, -8.2878e-01, -9.8684e-01,  3.4855e-01,\n",
       "                         2.7198e-01,  1.3012e-01,  1.6953e-01, -1.5300e-01,  1.7264e-02],\n",
       "                       [ 1.1099e-01, -4.0785e-01, -1.9722e-01, -1.3815e-01,  6.4919e-01,\n",
       "                         4.5842e-01, -9.2504e-01,  1.0310e-01, -1.9096e+00, -2.4210e+00,\n",
       "                         1.3677e-01, -5.0247e-01, -5.5440e-01, -5.8354e-01, -1.7761e-01,\n",
       "                         1.7059e-01, -4.0264e+00,  4.3869e-02, -2.1070e-02, -2.0081e-01],\n",
       "                       [-4.0591e-02, -1.7435e-01, -1.0490e-01,  3.8216e-01,  2.9472e-01,\n",
       "                         5.7125e-01, -1.3536e+00,  9.5585e-02, -5.0036e+00,  2.9042e+00,\n",
       "                         4.6429e-01, -6.4447e-01, -6.9692e-01, -5.6795e-01,  1.7429e-01,\n",
       "                         5.3728e-01, -1.0063e+00,  1.5847e-01, -3.0612e-01,  1.4985e-01],\n",
       "                       [ 5.8041e-02, -3.0590e-01, -1.6764e-01,  6.0778e-02,  3.6422e-01,\n",
       "                         5.1909e-01, -3.1047e+00,  1.5185e-01, -4.9592e-01,  2.2595e+00,\n",
       "                         2.6368e-01, -1.0244e+00, -1.2099e+00, -7.5868e-01, -2.2799e-01,\n",
       "                         3.4559e-01, -8.1067e-01,  5.6952e-02, -1.5691e-01, -1.2689e-01],\n",
       "                       [ 1.5404e-01,  1.8267e-01, -1.1484e-01,  1.5467e-01, -1.1752e-01,\n",
       "                         7.0486e-01, -4.5190e+00,  9.0493e-02,  4.2016e+00,  5.9512e-01,\n",
       "                         1.7206e-01, -9.6599e-01, -1.4417e+00, -5.0841e-01, -3.2581e-01,\n",
       "                         4.1972e-01, -1.2092e-01,  1.5065e-01, -1.7621e-01, -1.8691e-01],\n",
       "                       [ 7.1937e-02, -6.2027e-02, -3.0024e-01,  3.0641e-01,  1.2369e-01,\n",
       "                         6.4595e-01, -1.8433e-01,  6.4280e-02,  2.2645e+00,  6.2555e-01,\n",
       "                         1.9525e-01, -1.4999e+00, -3.5883e+00, -8.4549e-01, -5.2190e-02,\n",
       "                         5.9297e-01, -3.0589e-01,  4.0937e-02, -4.1632e-01, -8.6588e-02],\n",
       "                       [ 1.7646e-01,  1.8218e-01, -2.5701e-01,  2.6665e-01, -1.3046e-01,\n",
       "                         5.0526e-01,  3.8784e+00, -7.6283e-02,  6.3506e-01,  1.7642e-01,\n",
       "                         2.8324e-01, -1.8259e+00, -3.9382e+00, -8.0052e-01, -5.8429e-02,\n",
       "                         5.5301e-01, -4.3954e-03,  6.2647e-02, -4.1271e-01, -4.3139e-02],\n",
       "                       [ 5.8299e-02, -3.9062e-02, -3.5806e-01,  1.9565e-01,  5.5979e-02,\n",
       "                         3.3808e-01,  2.1516e+00, -7.7960e-02,  6.3477e-01,  2.7470e-01,\n",
       "                         7.8768e-03, -3.3048e+00, -4.0567e-01, -1.0778e+00,  1.2588e-01,\n",
       "                         3.7196e-01, -1.7203e-01,  4.6393e-02, -1.2353e-01,  1.2270e-02],\n",
       "                       [ 5.4804e-02,  1.8434e-01, -3.3819e-01,  2.0280e-01, -1.3469e-01,\n",
       "                         1.8105e-01,  6.7292e-01, -7.4404e-02,  2.7443e-01,  5.1565e-02,\n",
       "                        -1.5062e-01, -3.7376e+00,  2.9587e+00, -1.1898e+00,  1.5893e-01,\n",
       "                         1.5663e-01,  7.6135e-02, -1.7162e-02, -3.6039e-02,  4.0087e-02],\n",
       "                       [-1.3037e-01,  5.9412e-03, -1.5117e-01,  6.1569e-01, -2.6529e-02,\n",
       "                         1.1228e-01,  6.6360e-01, -9.1944e-02,  2.9145e-01,  1.6799e-01,\n",
       "                         5.2441e-02, -1.6825e+00,  2.9424e+00, -2.0862e+00,  6.8952e-01,\n",
       "                         2.7257e-01, -4.5408e-02, -1.6033e-02, -1.4819e-01,  2.8955e-01],\n",
       "                       [-1.3556e-01, -6.0371e-02,  2.1466e-03,  6.8529e-01,  1.0840e-01,\n",
       "                        -8.5225e-02,  5.5301e-01, -2.7459e-02,  3.5206e-01,  2.3065e-01,\n",
       "                         5.6778e-03,  8.1052e-01,  1.2716e+00, -2.9279e+00,  7.4407e-01,\n",
       "                         1.2094e-01, -1.3127e-01, -5.9937e-03, -5.9850e-02,  3.1070e-01],\n",
       "                       [-5.8541e-02,  1.4123e-02, -1.0452e-01,  2.2163e-01, -4.6775e-02,\n",
       "                        -2.0555e-01,  3.7217e-01, -3.7784e-02,  2.1402e-01,  1.1950e-01,\n",
       "                        -1.0180e-01,  2.5701e+00,  7.1837e-01, -2.9057e+00,  4.3037e-01,\n",
       "                        -1.5872e-01, -2.0765e-02, -4.5338e-02,  6.1569e-02,  2.2426e-01],\n",
       "                       [-1.5061e-02,  1.5794e-02, -1.8959e-01,  1.0389e-01,  5.7991e-03,\n",
       "                        -2.4561e-01,  3.9303e-01, -1.0997e-01,  2.4328e-01,  1.1089e-01,\n",
       "                        -8.8246e-02,  3.0735e+00,  6.3198e-01, -2.2969e+00,  2.6078e-01,\n",
       "                        -1.6766e-01, -5.5267e-02, -8.6933e-02,  6.9405e-02,  1.5634e-01],\n",
       "                       [-1.5390e-01, -1.8705e-01,  2.7228e-01,  5.5630e-01,  2.2342e-01,\n",
       "                        -1.9775e-01,  5.7408e-01, -4.1016e-02,  4.3226e-01,  3.3819e-01,\n",
       "                         1.0747e-01,  2.6433e+00,  7.9511e-01, -6.6260e-01,  8.1106e-01,\n",
       "                        -1.0159e-01, -2.8125e-01, -9.5746e-02, -8.0207e-02,  4.5904e-01],\n",
       "                       [-3.3302e-01, -3.0502e-01,  4.7091e-01,  7.8345e-01,  2.7337e-01,\n",
       "                        -2.9516e-01,  6.7261e-01,  3.8485e-02,  5.6490e-01,  4.7214e-01,\n",
       "                         1.2780e-01,  1.5741e+00,  8.1859e-01,  1.1152e+00,  1.1293e+00,\n",
       "                        -2.5934e-02, -3.9467e-01, -3.0258e-02, -1.1092e-01,  6.3494e-01],\n",
       "                       [-2.6006e-01, -2.1527e-01,  4.8043e-01,  4.7897e-01,  2.1536e-01,\n",
       "                        -4.1332e-01,  5.9166e-01,  7.6040e-02,  4.6791e-01,  3.9642e-01,\n",
       "                        -3.0130e-02,  1.1468e+00,  7.0733e-01,  2.5938e+00,  7.9070e-01,\n",
       "                        -2.4882e-01, -3.1789e-01,  2.5220e-02, -2.7908e-02,  4.8955e-01],\n",
       "                       [ 9.9307e-03, -1.5245e-01,  4.4385e-01,  2.1398e-02,  1.8559e-01,\n",
       "                        -5.2461e-01,  5.7095e-01, -3.1495e-03,  4.3224e-01,  3.4756e-01,\n",
       "                        -1.1808e-01,  1.0475e+00,  6.3005e-01,  3.1062e+00,  1.5085e-01,\n",
       "                        -4.3170e-01, -2.6417e-01, -1.3486e-01,  1.6556e-01,  1.6346e-01],\n",
       "                       [ 1.4793e-01, -1.1269e-01,  1.4633e-01, -5.1086e-01,  1.3088e-01,\n",
       "                        -6.1957e-01,  5.0434e-01, -9.2081e-02,  3.9349e-01,  3.3327e-01,\n",
       "                        -3.0862e-01,  9.7243e-01,  6.1352e-01,  3.1763e+00, -7.1321e-01,\n",
       "                        -4.6702e-01, -2.2601e-01, -2.0812e-01,  4.0368e-01, -2.9772e-01],\n",
       "                       [ 2.4838e-01, -3.8883e-02, -1.1965e-01, -1.1470e+00,  9.9585e-02,\n",
       "                        -7.7014e-01,  4.4955e-01, -2.3272e-01,  4.0388e-01,  2.7734e-01,\n",
       "                        -5.7929e-01,  8.9655e-01,  5.4630e-01,  3.1361e+00, -1.4799e+00,\n",
       "                        -5.7239e-01, -1.9151e-01, -2.4811e-01,  5.4082e-01, -7.2291e-01],\n",
       "                       [ 2.2225e-01,  4.5843e-02, -1.4974e-01, -1.6460e+00,  3.8806e-02,\n",
       "                        -9.5908e-01,  4.2172e-01, -3.0276e-01,  3.0823e-01,  2.3923e-01,\n",
       "                        -7.7204e-01,  7.9470e-01,  5.0928e-01,  3.1327e+00, -1.9791e+00,\n",
       "                        -8.1925e-01, -1.5372e-01, -3.1454e-01,  6.6939e-01, -1.0773e+00]],\n",
       "                      device='cuda:0')),\n",
       "              ('agents.1.cell.weight_ih_l0',\n",
       "               tensor([[-0.0799, -0.0769, -0.0108,  ..., -0.0166, -0.0750, -0.0734],\n",
       "                       [-0.0558, -0.0522, -0.0231,  ..., -0.0309, -0.0516, -0.0569],\n",
       "                       [-0.0097, -0.0033,  0.0940,  ...,  0.0841, -0.0146, -0.0045],\n",
       "                       ...,\n",
       "                       [ 0.0238,  0.0281, -0.0289,  ..., -0.0252,  0.0223,  0.0223],\n",
       "                       [-0.0464, -0.0456, -0.0086,  ..., -0.0159, -0.0469, -0.0451],\n",
       "                       [-0.0274, -0.0230,  0.0628,  ...,  0.0563, -0.0217, -0.0213]],\n",
       "                      device='cuda:0')),\n",
       "              ('agents.1.cell.weight_hh_l0',\n",
       "               tensor([[ 5.5285e-01, -3.5379e-01, -2.0140e-01, -3.0997e-02,  1.9143e-01,\n",
       "                        -4.4883e-01, -8.0554e-02,  4.1549e-01, -9.5732e-01, -3.0321e-01,\n",
       "                         2.9438e-03, -2.0086e+00,  7.1610e-01,  2.0750e-01, -2.3779e-01,\n",
       "                         8.6540e-01,  8.4845e-01,  2.8165e-01,  1.4425e+00, -1.3965e-01],\n",
       "                       [ 1.2305e+00,  3.2384e-01, -1.4517e-01,  2.5478e-01,  3.6424e-01,\n",
       "                        -3.3051e-01, -2.0615e-01, -2.4238e+00, -7.4295e-01, -2.5903e-01,\n",
       "                         5.0718e-01, -8.6773e-01,  9.6820e-01, -2.6622e-02,  7.2912e-01,\n",
       "                         6.8476e-01, -1.4812e-02, -4.5127e-01,  8.5397e-01, -4.4239e-02],\n",
       "                       [ 2.6203e-01,  3.0189e-01,  3.3079e-01, -2.8820e-01,  4.0178e-02,\n",
       "                         5.0748e-01, -4.9811e-01, -2.2112e-01,  3.2700e-01,  1.7089e-01,\n",
       "                         5.5336e-01, -2.8092e-02,  1.1786e-01, -1.3749e+00,  8.7669e-01,\n",
       "                        -1.3217e-02, -5.8471e-01, -8.0356e-01,  1.4699e-01,  2.1829e+00],\n",
       "                       [-3.0974e-01, -5.1535e-01, -6.2726e-02,  1.3281e+00,  2.9686e-01,\n",
       "                        -5.9794e-01,  7.1419e-01,  2.0759e-01,  7.1342e-01,  9.7635e-01,\n",
       "                         1.1838e+00,  4.8754e-01, -2.9738e-01,  4.5615e-01, -8.8277e-01,\n",
       "                        -3.7977e-01, -8.2111e-01, -2.2925e-02, -2.2445e-01,  4.8341e-02],\n",
       "                       [ 4.3092e-01,  4.7171e-01,  7.8654e-01, -1.5802e+00,  1.1008e+00,\n",
       "                         2.3562e-01,  1.8822e-01, -3.2377e-01, -7.1370e-01,  9.8413e-02,\n",
       "                        -1.0826e+00, -5.5908e-01, -1.0574e+00, -1.0147e-01, -1.6871e-01,\n",
       "                         4.2821e-01,  8.1380e-01,  1.3747e-01,  4.3584e-01,  2.8617e-01],\n",
       "                       [ 1.0409e+00,  1.6731e+00, -7.6344e-03,  9.2857e-02,  1.2570e-01,\n",
       "                         4.4658e-01,  1.1604e-01, -1.7023e+00, -1.0616e+00, -1.6649e-02,\n",
       "                         1.3498e-01, -1.3209e+00,  1.0085e+00,  2.1061e-01,  4.6135e-01,\n",
       "                         1.1039e+00,  2.6330e-01, -1.0371e-01,  9.8070e-01, -3.9837e-02],\n",
       "                       [ 1.5619e-01,  8.5951e-02,  2.7971e-01, -1.3093e+00,  9.6592e-01,\n",
       "                        -1.4518e-01,  6.7738e-01, -7.6910e-02, -7.5373e-01, -1.6658e-01,\n",
       "                        -2.5125e+00, -2.8974e-01, -8.7985e-01, -2.7725e-01, -8.3192e-02,\n",
       "                         2.2522e-01,  7.6165e-01,  3.2408e-01,  1.5931e-01,  2.4964e-01],\n",
       "                       [-2.4899e+00,  1.4707e-02, -2.3878e-01,  4.0308e-01, -2.7231e-01,\n",
       "                         4.7622e-01,  2.3716e-01,  1.0030e+00,  5.8702e-01,  3.0431e-01,\n",
       "                        -3.0328e-01,  3.1652e-01, -1.0108e+00, -8.2276e-03, -2.0987e-01,\n",
       "                        -5.0868e-01, -2.4734e-01, -2.8302e-01, -1.0795e+00, -5.2108e-02],\n",
       "                       [ 5.2318e-01,  7.9476e-01, -9.9996e-01, -1.5268e-01, -1.9649e-01,\n",
       "                         8.6057e-01,  1.8455e-01, -7.2287e-01,  1.3586e+00, -2.5747e-01,\n",
       "                         4.0623e-01, -1.7390e-01, -6.2627e-03,  2.2878e-01, -2.7538e-02,\n",
       "                        -6.8713e-03, -1.4022e+00, -1.4818e-01,  3.4560e-01, -6.3382e-01],\n",
       "                       [-3.6886e-01, -5.5081e-01,  1.2789e-01, -5.2838e-01,  1.2752e+00,\n",
       "                        -5.0063e-01,  2.3299e-01,  3.6825e-01,  1.0724e+00,  1.5914e+00,\n",
       "                        -8.7737e-01,  6.9813e-01, -1.0026e+00,  1.4760e-02, -3.7549e-01,\n",
       "                        -7.3100e-01, -1.0274e+00, -1.0264e+00, -4.1273e-01, -1.9780e-01],\n",
       "                       [-5.7070e-03, -8.2529e-03, -1.8820e+00, -5.3003e-01,  5.0466e-01,\n",
       "                         1.9594e-01,  2.1958e-02, -1.2815e-02, -8.2721e-02,  6.2871e-01,\n",
       "                         1.6844e+00, -1.0332e-02,  3.1248e-01,  6.4454e-01, -1.5550e-01,\n",
       "                        -2.3510e-03, -2.8332e-01, -1.1183e+00,  5.9402e-02, -8.9574e-01],\n",
       "                       [ 3.6530e-01,  6.3552e-01, -8.6995e-01, -2.6111e-01, -1.1106e-01,\n",
       "                         7.0053e-01,  7.1378e-02, -4.5741e-01,  1.1336e+00, -4.6894e-01,\n",
       "                         4.3053e-01,  1.4264e+00, -4.8702e-02,  2.4760e-01,  9.7397e-02,\n",
       "                        -1.8237e+00, -1.2328e+00,  1.0355e-02,  1.7297e-01, -5.4872e-01],\n",
       "                       [ 1.7531e-01, -5.0799e-01,  8.6631e-02,  1.5440e+00, -7.8652e-01,\n",
       "                        -5.9899e-01, -4.4651e-02,  1.5930e-01, -4.8630e-01,  6.3479e-01,\n",
       "                         1.4556e+00, -5.1084e-01,  5.8572e-01,  1.2669e-01, -1.0574e-02,\n",
       "                         1.1351e+00, -2.4703e-01,  3.4549e-01,  3.3163e-01, -1.1862e-01],\n",
       "                       [ 4.9725e-02, -5.9647e-01,  2.5295e-01, -1.8227e-01, -7.0639e-02,\n",
       "                        -7.9547e-01,  8.5651e-01,  2.0927e-01, -4.4714e-01, -1.0833e-01,\n",
       "                        -1.0333e+00,  5.1233e-02,  9.1041e-02,  6.5804e-01, -2.7009e+00,\n",
       "                         4.0607e-01,  7.9883e-02,  4.9833e-01, -3.7725e-02, -7.4825e-02],\n",
       "                       [-8.7281e-04,  1.0029e+00, -4.5927e-01,  1.9601e-01, -9.5596e-02,\n",
       "                         1.2386e+00, -9.6748e-01, -5.1856e-01,  7.9393e-01, -1.7726e-02,\n",
       "                         1.0782e+00,  1.8766e-01,  2.5887e-01,  2.3438e-01,  9.8883e-01,\n",
       "                        -9.6447e-02, -4.1729e-01, -3.3052e-01, -1.8741e-01, -2.4336e-01],\n",
       "                       [-6.2164e-01, -8.4301e-01,  9.6921e-01,  8.9863e-02,  2.6866e-01,\n",
       "                        -9.3961e-01, -1.6812e-01,  8.6790e-01, -1.0122e+00,  2.7170e-01,\n",
       "                        -3.3112e-01,  1.6190e-01, -2.6907e-02, -3.2307e-01,  2.5666e-02,\n",
       "                         1.5313e+00,  8.0594e-01,  1.4452e-01, -5.1299e-01,  7.0614e-01],\n",
       "                       [-5.5518e-01, -8.5463e-01,  9.0905e-01,  2.8112e-01, -9.2097e-02,\n",
       "                        -1.0513e+00, -2.1576e-01,  6.8625e-01, -6.6806e-02,  3.9258e-01,\n",
       "                        -3.9644e-01,  2.6834e-01,  2.9839e-02, -4.1571e-01,  1.3413e-01,\n",
       "                        -1.3802e-01,  1.1071e+00, -9.1765e-04, -4.1798e-01,  7.8896e-01],\n",
       "                       [ 1.6419e-01,  3.1940e-01,  1.3646e+00,  5.4090e-02, -5.7360e-01,\n",
       "                         2.4315e-01, -5.5208e-01, -1.0194e-01, -4.3298e-01,  3.3274e-01,\n",
       "                         2.5082e-01, -3.3393e-01,  4.8954e-01, -7.6592e-01,  5.5661e-01,\n",
       "                         3.1608e-01,  6.7849e-01,  1.3060e+00,  2.0749e-01,  1.3406e+00],\n",
       "                       [-1.6561e-01, -3.5685e-01,  1.3286e-01,  1.8048e-01,  2.2551e-01,\n",
       "                        -4.9842e-01, -1.9288e-02,  4.6589e-01, -9.2084e-01, -9.2110e-02,\n",
       "                        -1.8434e-01, -2.3365e+00,  2.9348e-01,  8.5701e-02, -4.7950e-02,\n",
       "                         8.8097e-01,  8.5120e-01,  1.9400e-02,  1.1599e+00, -1.0178e-01],\n",
       "                       [ 9.4478e-02,  3.8253e-01, -5.2345e-02,  1.7447e-01, -1.8300e-01,\n",
       "                         6.1027e-01, -5.8143e-01, -2.1218e-01, -1.9765e-01,  1.9241e-01,\n",
       "                         6.5549e-01,  7.1764e-02, -8.8291e-02, -2.2933e+00,  1.3820e+00,\n",
       "                        -1.8228e-01, -3.5742e-01, -2.3864e-01,  1.3982e-01,  5.8737e-01]],\n",
       "                      device='cuda:0')),\n",
       "              ('agents.1.readout.0.weight',\n",
       "               tensor([[ 2.5639e-01, -3.3068e-03, -1.8349e+00,  5.5411e-01, -8.7359e-01,\n",
       "                        -2.4917e-01, -1.0362e+00, -1.4246e-01,  1.0206e+00, -1.5356e+00,\n",
       "                         1.8995e+00,  1.4903e+00, -2.8864e-02,  1.1114e+00, -4.5759e-01,\n",
       "                        -6.6471e-01, -1.0130e+00, -1.1105e+00, -5.9810e-02, -1.3675e+00],\n",
       "                       [-1.3916e-01, -1.0061e-01, -2.0656e+00, -1.2233e+00,  1.3494e+00,\n",
       "                        -2.0410e-01,  1.8569e+00,  1.3436e-01,  1.8161e-01,  4.1764e-01,\n",
       "                         1.9949e+00, -8.4683e-01, -1.0135e+00,  8.9236e-01, -1.0239e+00,\n",
       "                        -5.8419e-01, -5.8161e-01, -8.1683e-01,  5.4262e-02, -8.6461e-01],\n",
       "                       [-1.0975e-01, -7.4673e-02, -1.3410e+00, -7.7432e-01,  1.9414e+00,\n",
       "                        -4.1778e-01,  9.9144e-01,  7.5526e-02,  1.3090e-01,  2.6299e-01,\n",
       "                        -2.8326e+00, -2.3103e-01, -1.7721e-01,  1.3103e+00, -9.4396e-01,\n",
       "                        -1.4017e-01, -8.8811e-01, -6.5705e-01, -9.6341e-02, -1.7952e+00],\n",
       "                       [ 1.2318e-01,  3.5790e-01,  2.5548e+00, -2.5516e-01,  1.5492e+00,\n",
       "                         2.7993e-01,  1.2030e+00, -2.8355e-01,  1.9110e-01,  2.6666e-02,\n",
       "                        -1.8461e+00, -1.1133e-01, -8.2817e-01,  1.2969e+00, -5.6626e-01,\n",
       "                         1.3293e-01, -2.9262e-01,  2.1842e+00,  9.2874e-02, -2.5121e+00],\n",
       "                       [ 6.1556e-02,  4.1133e-01,  1.3611e+00, -5.0916e-01,  1.4431e+00,\n",
       "                         5.6424e-01,  1.6708e+00, -1.3802e-01,  8.4535e-03,  1.2577e-01,\n",
       "                        -1.2599e+00, -1.0019e-01, -8.3639e-01,  2.6944e+00, -1.5837e+00,\n",
       "                         1.5181e-01, -3.5682e-01,  1.2021e+00, -7.7986e-03,  2.5936e+00],\n",
       "                       [ 4.9592e-02,  3.0804e-01,  7.8391e-01, -7.6410e-01,  1.4540e+00,\n",
       "                         4.1206e-01,  2.1558e+00, -9.8747e-02, -4.7885e-01,  5.5075e-02,\n",
       "                        -8.7289e-01, -8.4489e-02, -1.0340e+00, -2.5360e+00, -3.3494e+00,\n",
       "                         1.9310e-01, -7.9616e-02,  6.8396e-01,  7.8273e-02,  1.2362e+00],\n",
       "                       [ 8.1343e-02, -3.1896e-01,  2.3689e-01, -9.8094e-01,  1.6523e+00,\n",
       "                        -5.6382e-01,  3.3495e+00,  1.3617e-01, -3.6137e-01,  2.8147e-01,\n",
       "                        -4.8281e-01, -3.6106e-01, -1.5305e+00, -1.4641e+00,  1.2695e+00,\n",
       "                         4.1708e-01,  6.2209e-01,  1.8160e-01,  2.4650e-01,  5.8585e-01],\n",
       "                       [ 2.1476e-02, -2.7806e-01,  1.6870e-01, -1.1741e+00,  3.2376e+00,\n",
       "                        -2.0663e-01, -1.9459e-01,  1.0973e-01, -5.5152e-01, -5.2779e-01,\n",
       "                        -6.2347e-01, -1.3738e-01, -1.4631e+00, -7.6224e-01,  1.5744e+00,\n",
       "                         4.6722e-01,  3.7926e-01,  1.6984e-01, -2.4784e-02,  3.9870e-01],\n",
       "                       [-2.9892e-03, -1.2511e-01, -2.6153e-01, -1.2307e+00,  3.4422e+00,\n",
       "                        -2.8489e-01, -2.7004e+00,  1.6317e-03, -2.7609e-01, -6.0637e-01,\n",
       "                        -3.5939e-01,  1.4282e-01,  5.1250e-01, -1.2670e-01,  8.4118e-01,\n",
       "                         4.2165e-02,  4.1891e-01, -1.2454e-01, -4.6226e-02, -4.6955e-02],\n",
       "                       [-3.7338e-02, -1.6446e-01, -1.5266e-01, -3.0464e+00,  1.4400e-01,\n",
       "                        -2.3739e-01, -1.9747e+00,  1.1421e-01, -4.4574e-02, -6.9566e-01,\n",
       "                        -4.6910e-01,  1.7193e-02,  1.1015e+00, -8.1152e-02,  6.1411e-01,\n",
       "                        -7.5291e-02,  3.6663e-01,  3.7581e-02,  4.2737e-02,  6.0199e-05],\n",
       "                       [-4.4555e-02, -1.0883e-01, -1.3769e-01, -4.3078e+00, -3.5089e+00,\n",
       "                        -1.3093e-01, -4.0424e-01,  1.1688e-01,  2.8301e-01, -5.7711e-01,\n",
       "                        -1.0826e+00, -2.4992e-02,  2.1380e-01,  5.9838e-02,  2.2907e-01,\n",
       "                        -1.3107e-01,  5.5346e-01,  6.8344e-02, -2.4910e-02, -1.2199e-01],\n",
       "                       [-4.1753e-02, -1.1813e-02, -4.0488e-02, -1.1639e+00, -3.3605e+00,\n",
       "                         2.1760e-03, -3.5657e-01,  1.5547e-01,  2.3164e-01, -8.9421e-01,\n",
       "                        -2.5258e+00, -9.4282e-02,  3.4245e-01, -2.9591e-02,  2.2390e-01,\n",
       "                        -1.4095e-01,  2.3996e-01,  3.1161e-01, -6.7132e-02, -3.4393e-04],\n",
       "                       [-8.3311e-02, -3.2440e-02, -1.2818e-01,  2.0745e+00, -1.5305e+00,\n",
       "                         1.5315e-01, -2.4423e-01,  6.6125e-02,  1.0364e-01, -1.0520e+00,\n",
       "                        -4.3254e+00, -1.2833e-01,  2.5946e-01,  2.6066e-03,  1.4958e-01,\n",
       "                        -1.2759e-02, -2.9347e-01,  3.6637e-01, -9.4634e-03, -4.6549e-02],\n",
       "                       [-7.3370e-02,  5.1568e-02, -1.6712e-01,  3.8816e+00, -9.9198e-01,\n",
       "                         8.5230e-02, -1.0861e-01,  2.2857e-02, -7.3655e-02, -1.4469e+00,\n",
       "                        -3.0334e+00,  1.6812e-02,  2.3042e-01,  4.7115e-02,  2.8192e-02,\n",
       "                         7.0783e-02, -4.9425e-01,  4.7180e-01, -2.1327e-02, -5.7025e-02],\n",
       "                       [ 2.7230e-02, -2.2588e-02, -7.3901e-02,  2.5412e+00, -8.6325e-01,\n",
       "                         1.8539e-01, -1.4714e-01, -1.1734e-02,  4.9292e-02, -3.0605e+00,\n",
       "                        -1.4636e-01,  1.9188e-01,  2.1308e-01, -9.0969e-03,  7.7534e-02,\n",
       "                        -1.0512e-01,  1.8261e-01,  7.4914e-01, -1.5078e-02,  2.6587e-02],\n",
       "                       [ 5.6370e-03, -3.5045e-03, -1.9570e-01,  1.1867e+00, -5.6345e-01,\n",
       "                         2.5267e-01,  3.3753e-02, -3.2221e-02,  4.1284e-02, -3.9113e+00,\n",
       "                         2.7260e+00,  2.2898e-01,  3.9125e-02,  8.7633e-02, -3.6187e-02,\n",
       "                        -1.6823e-01,  6.4489e-01,  8.6250e-01, -1.1999e-02, -8.3661e-02],\n",
       "                       [-2.0733e-02,  6.9054e-02, -1.3871e-01,  9.0921e-01, -5.0848e-01,\n",
       "                         2.0999e-01, -3.4031e-02,  1.6343e-02,  9.4898e-02, -2.3768e+00,\n",
       "                         3.7649e+00,  1.3896e-01,  8.1549e-02,  3.5511e-02,  9.8241e-03,\n",
       "                        -2.5778e-01,  6.7792e-01,  1.4241e+00, -3.7035e-02, -1.0275e-01],\n",
       "                       [ 1.8142e-02,  8.3401e-02, -1.0631e-03,  7.5751e-01, -5.3490e-01,\n",
       "                         1.9579e-01, -1.5685e-01,  4.8014e-02,  9.1271e-02, -1.3774e-01,\n",
       "                         2.6207e+00,  4.9891e-02,  2.4425e-01, -4.1178e-02,  1.3596e-01,\n",
       "                        -9.0490e-02,  2.0415e-01,  2.7669e+00, -1.0873e-01,  3.9400e-02],\n",
       "                       [-9.1306e-02,  1.1228e-01, -1.3734e-02,  5.4779e-01, -3.8785e-01,\n",
       "                         1.2975e-01, -1.5355e-01, -6.6135e-02, -5.8195e-03,  2.0077e+00,\n",
       "                         1.3781e+00,  6.4281e-02,  2.6253e-01, -9.5639e-02,  1.4354e-01,\n",
       "                         3.9564e-03, -1.2789e-01,  3.5495e+00, -2.2515e-04,  8.2044e-02],\n",
       "                       [ 4.5838e-02,  8.3421e-02,  1.7683e-02,  3.6714e-01, -3.1337e-01,\n",
       "                         2.2899e-01, -1.6817e-01,  5.3180e-02, -6.3301e-03,  3.6069e+00,\n",
       "                         1.0296e+00, -4.7497e-02,  2.6006e-01, -7.4377e-02,  2.1682e-01,\n",
       "                        -2.1387e-02, -1.1032e-01,  2.7875e+00, -9.2724e-02,  9.2747e-02],\n",
       "                       [-8.4361e-02,  5.8182e-03,  2.2993e-01,  4.2026e-01, -4.4159e-01,\n",
       "                         3.7023e-02, -4.3240e-01, -1.1553e-01,  9.2249e-02,  2.9644e+00,\n",
       "                         1.0211e+00,  2.0925e-02,  5.0076e-01, -3.5974e-01,  4.3222e-01,\n",
       "                         2.4564e-02, -1.7154e-01,  6.4155e-01, -7.7462e-02,  3.7162e-01],\n",
       "                       [ 2.2514e-03,  4.0101e-02,  3.7788e-01,  4.8811e-01, -5.0961e-01,\n",
       "                        -2.9487e-02, -5.8293e-01, -5.4407e-02,  3.4187e-02,  1.8774e+00,\n",
       "                         8.9146e-01,  3.6081e-02,  6.0424e-01, -5.3697e-01,  5.7464e-01,\n",
       "                         3.6210e-02, -2.0903e-01, -1.1552e+00,  3.2034e-02,  4.9631e-01],\n",
       "                       [-2.9244e-02, -1.2158e-02,  3.5620e-01,  3.3465e-01, -4.4585e-01,\n",
       "                         1.1555e-02, -5.0908e-01,  1.1722e-02, -2.6509e-03,  1.2930e+00,\n",
       "                         7.0524e-01, -7.9096e-02,  5.9073e-01, -5.6025e-01,  5.4904e-01,\n",
       "                         9.2713e-02, -1.6439e-01, -2.9000e+00,  3.1570e-02,  4.5732e-01],\n",
       "                       [ 1.3773e-02,  1.2960e-02,  3.0288e-01,  2.4932e-01, -3.7988e-01,\n",
       "                         4.8812e-02, -4.8428e-01, -5.6511e-03, -9.7776e-02,  1.0796e+00,\n",
       "                         6.1495e-01, -5.3264e-02,  5.5040e-01, -4.9156e-01,  5.0860e-01,\n",
       "                         1.1109e-01, -2.3455e-01, -3.8427e+00, -5.4725e-03,  4.3966e-01],\n",
       "                       [-5.5049e-02, -5.0448e-02,  2.6891e-01,  1.7475e-01, -3.4195e-01,\n",
       "                        -1.0017e-02, -4.2294e-01, -6.2450e-02, -1.9783e-01,  9.9953e-01,\n",
       "                         5.4573e-01,  1.2726e-03,  5.7672e-01, -4.5854e-01,  4.8770e-01,\n",
       "                         7.5975e-02, -3.2334e-01, -4.2409e+00,  1.2441e-01,  4.5968e-01]],\n",
       "                      device='cuda:0')),\n",
       "              ('agents.1.readout.1.weight',\n",
       "               tensor([[ 6.7105e-01,  9.9142e-01,  4.8720e-01, -3.2285e-02,  1.8817e-01,\n",
       "                         1.9130e+00, -6.4450e-03, -8.0063e-01,  2.4337e+00, -1.4597e-02,\n",
       "                        -3.3478e-01, -3.6653e-01, -4.7085e-01, -6.7161e-01,  1.3929e+00,\n",
       "                        -8.0137e-01, -2.3154e+00,  2.6318e-01,  5.1391e-01,  5.7105e-01],\n",
       "                       [ 8.3388e-01,  1.0593e+00,  5.0716e-02, -4.1684e-01, -6.7398e-02,\n",
       "                         2.6736e+00, -1.4393e-01, -9.1061e-01, -1.2095e+00, -4.6217e-01,\n",
       "                         7.8853e-01, -3.8334e-01,  5.7995e-01,  2.0415e-01,  3.3381e-01,\n",
       "                         3.2306e-01, -2.5215e+00,  1.9078e-01,  8.5104e-01, -1.0802e-01],\n",
       "                       [ 9.8919e-01,  1.2712e+00, -7.1815e-01, -1.2791e-01, -1.1143e-01,\n",
       "                         3.9937e+00, -1.8616e-01, -1.0431e+00, -8.4993e-01, -5.4030e-01,\n",
       "                         6.7575e-01, -4.0126e-01,  1.2170e+00,  4.9031e-01, -8.8081e-01,\n",
       "                         4.6826e-01,  2.4994e+00, -7.2788e-01,  8.3759e-01, -5.1557e-01],\n",
       "                       [ 1.2510e+00,  2.7209e+00, -2.5462e-01, -4.9767e-02, -1.3032e-01,\n",
       "                        -4.6961e-01,  1.3413e-01, -1.5273e+00, -4.9790e-01, -3.2632e-01,\n",
       "                         4.9543e-01, -6.3882e-01,  8.7961e-01,  3.8118e-01, -3.2770e-01,\n",
       "                         5.0126e-01,  9.6886e-01, -4.1252e-01,  1.1156e+00, -2.1843e-01],\n",
       "                       [ 1.0035e+00,  1.8312e+00, -2.5055e-01,  1.8772e-02, -2.7171e-01,\n",
       "                        -4.2927e+00,  2.8669e-01, -1.5856e+00, -4.6536e-01, -1.5398e-01,\n",
       "                         3.3883e-01, -7.1038e-01,  3.4582e-01,  7.2746e-02, -1.7680e-01,\n",
       "                         2.8689e-01,  5.2079e-01, -1.3451e-01,  8.3073e-01, -2.7366e-01],\n",
       "                       [ 1.5663e+00, -3.4766e+00, -4.3990e-01,  3.5082e-01, -6.5957e-01,\n",
       "                        -1.2545e+00,  3.7799e-01, -3.5171e+00, -4.9695e-01, -6.4166e-02,\n",
       "                        -3.2450e-02, -7.3769e-01,  2.0346e-01,  2.3251e-01, -3.4570e-01,\n",
       "                         3.0723e-01,  5.7219e-01, -2.6246e-01,  9.1869e-01, -2.1880e-01],\n",
       "                       [ 4.2598e+00, -1.5595e+00, -3.7241e-01,  8.1291e-02, -5.0650e-01,\n",
       "                        -8.7756e-01,  2.1805e-01,  1.7408e+00, -5.2954e-01,  1.8366e-01,\n",
       "                         1.2030e-02, -7.5347e-01,  1.1460e-01,  2.6993e-01, -2.4199e-01,\n",
       "                         5.5947e-01,  6.1633e-01, -3.4579e-01,  1.1807e+00, -1.9326e-01],\n",
       "                       [ 7.9426e-01, -1.0673e+00, -3.7606e-01,  2.8649e-01, -4.8998e-01,\n",
       "                        -6.3120e-01,  2.2180e-01,  2.6249e+00, -6.3948e-01, -4.2441e-02,\n",
       "                         4.1653e-02, -9.4653e-01,  1.5524e-01,  3.2880e-01, -3.1288e-01,\n",
       "                         6.6628e-01,  7.1697e-01, -3.0951e-01,  2.4123e+00, -3.7587e-01],\n",
       "                       [-3.7052e+00, -3.9078e-01, -6.0132e-01,  3.6451e-01, -6.9080e-01,\n",
       "                        -1.2400e-01,  1.9698e-01,  8.6096e-01, -5.4257e-01,  5.1216e-02,\n",
       "                        -2.6973e-01, -1.0311e+00, -1.4676e-01,  4.3463e-01, -3.3688e-01,\n",
       "                         6.3236e-01,  5.9759e-01, -1.9812e-01,  4.5090e+00, -5.4757e-01],\n",
       "                       [-2.5995e+00, -6.3099e-01, -5.0719e-01,  2.7297e-01, -6.2666e-01,\n",
       "                        -3.8945e-01, -4.5966e-02,  9.0958e-01, -7.0075e-01, -2.0499e-01,\n",
       "                        -9.6366e-02, -3.1217e+00,  8.2349e-02,  5.5724e-01, -5.0874e-01,\n",
       "                         1.0283e+00,  7.0868e-01, -6.2719e-01,  1.4775e-01, -5.3320e-01],\n",
       "                       [-8.5184e-01, -2.0753e-01, -6.2459e-01,  3.5032e-01, -3.6646e-01,\n",
       "                        -1.0101e-02, -2.2568e-01,  3.7197e-01, -8.3605e-01, -1.3854e-01,\n",
       "                        -5.0354e-02, -4.1259e+00, -2.0211e-02,  5.2556e-01, -5.2319e-01,\n",
       "                         1.2171e+00,  5.7794e-01, -7.3980e-01, -4.0234e+00, -5.5493e-01],\n",
       "                       [-8.8984e-01, -3.7610e-01, -6.6046e-01,  2.4645e-01, -3.8537e-01,\n",
       "                        -2.3196e-01, -5.7921e-02,  5.1067e-01, -1.4380e+00,  2.9952e-02,\n",
       "                        -1.1491e-01, -1.9514e-01, -1.6724e-01,  4.5453e-01, -3.4985e-01,\n",
       "                         3.2927e+00,  8.2801e-01, -6.6810e-01, -2.5569e+00, -6.8388e-01],\n",
       "                       [-4.5008e-01, -1.2061e-01, -6.8109e-01,  2.6779e-01, -2.1456e-01,\n",
       "                         3.3989e-02, -2.4320e-01,  2.3199e-01, -1.4593e+00,  3.4483e-01,\n",
       "                        -3.2203e-01,  3.7387e+00, -3.2451e-01,  4.1396e-01, -2.8605e-01,\n",
       "                         4.1057e+00,  8.6956e-01, -6.6874e-01, -8.7807e-01, -6.2211e-01],\n",
       "                       [-5.0880e-01, -1.7932e-01, -2.9643e-01,  5.1902e-01,  9.1515e-02,\n",
       "                        -3.2476e-02, -1.9807e-01,  2.8168e-01, -2.6384e+00,  2.9138e-01,\n",
       "                        -4.0805e-01,  2.6711e+00, -3.9034e-01,  1.7723e-01, -1.9416e-01,\n",
       "                         1.0273e+00,  9.4267e-01, -4.7309e-01, -8.6670e-01, -2.9143e-01],\n",
       "                       [-3.4732e-01, -9.8230e-02, -1.6238e-01,  5.9754e-01,  4.2635e-01,\n",
       "                        -9.7640e-03, -8.2286e-02,  2.1113e-01, -3.1979e+00,  2.3869e-01,\n",
       "                        -3.3591e-01,  1.1762e+00, -1.6852e-01,  1.1240e-01, -6.6641e-02,\n",
       "                        -1.9121e+00,  6.7864e-01, -2.7157e-01, -6.0785e-01, -8.0401e-02],\n",
       "                       [-2.3963e-01,  1.8193e-02, -2.0694e-01,  7.6067e-01,  4.3265e-01,\n",
       "                         1.1544e-01, -1.6571e-01,  6.4217e-02, -2.6036e+00,  3.7609e-01,\n",
       "                        -5.5444e-01,  8.0584e-01, -4.3580e-01, -1.2006e-01,  6.0854e-02,\n",
       "                        -3.1600e+00,  9.5263e-01, -2.5434e-01, -3.6522e-01,  6.6685e-03],\n",
       "                       [-2.9770e-01, -4.5090e-02, -1.4312e-01,  6.6566e-01,  6.2241e-01,\n",
       "                        -1.8639e-02, -2.7889e-01,  1.7282e-01, -4.5160e-01,  4.7542e-01,\n",
       "                        -5.1059e-01,  7.2022e-01, -4.0575e-01, -1.2171e-01,  1.8397e-01,\n",
       "                        -2.2862e+00,  2.1118e+00, -2.0120e-03, -4.8444e-01,  1.1065e-01],\n",
       "                       [-2.8062e-01, -1.0277e-01, -7.4388e-02,  3.9034e-01,  8.1438e-01,\n",
       "                         4.4524e-03, -2.3873e-01,  1.5736e-01,  1.7207e+00,  4.0168e-01,\n",
       "                        -3.5056e-01,  5.8756e-01, -1.6475e-01, -1.2578e-01,  2.1388e-01,\n",
       "                        -1.2151e+00,  2.5781e+00,  1.9665e-01, -3.9800e-01,  5.2523e-02],\n",
       "                       [-1.5422e-01, -1.1660e-02,  2.3729e-01,  2.7593e-01,  7.3161e-01,\n",
       "                         7.9497e-02, -1.2956e-01,  8.7325e-02,  3.0772e+00,  3.8219e-01,\n",
       "                        -3.2108e-01,  4.1890e-01, -1.1565e-01, -8.0878e-02,  1.8256e-01,\n",
       "                        -8.5513e-01,  2.3669e+00,  2.2234e-01, -2.7507e-01,  5.9181e-02],\n",
       "                       [-1.4135e-01, -2.2654e-02,  6.1751e-01,  2.5888e-01,  7.7324e-01,\n",
       "                         1.5120e-01, -8.3110e-02,  2.9325e-02,  3.1040e+00,  4.1889e-01,\n",
       "                        -2.7874e-01,  4.3403e-01, -1.3320e-02, -2.6973e-01,  2.8328e-01,\n",
       "                        -7.4515e-01,  1.2183e+00,  4.4274e-01, -2.8038e-01,  2.9740e-01],\n",
       "                       [-2.1526e-01, -1.0770e-01,  6.5139e-01, -1.9766e-01,  4.3177e-01,\n",
       "                         7.4378e-02, -1.3078e-01,  1.3079e-01,  2.2142e+00,  4.8735e-01,\n",
       "                        -2.4101e-01,  4.9157e-01,  1.4827e-01, -5.2039e-01,  3.2738e-01,\n",
       "                        -7.5931e-01, -5.5813e-01,  7.6908e-01, -3.4409e-01,  4.8935e-01],\n",
       "                       [-2.6855e-01, -7.5060e-02,  6.9132e-01, -7.7171e-01, -2.9642e-02,\n",
       "                         6.8263e-02, -1.0486e-01,  1.4014e-01,  1.4177e+00,  2.9077e-01,\n",
       "                        -4.6832e-02,  4.8051e-01,  1.5917e-01, -5.3430e-01,  4.4235e-01,\n",
       "                        -6.3431e-01, -2.2549e+00,  8.8567e-01, -3.5861e-01,  5.5370e-01],\n",
       "                       [-1.9993e-01, -5.3968e-02,  7.9518e-01, -1.0090e+00, -1.7898e-01,\n",
       "                         1.4749e-01, -1.3546e-02,  8.9394e-02,  1.0898e+00, -2.1063e-01,\n",
       "                         2.8682e-01,  3.7075e-01,  3.2352e-02, -5.7847e-01,  3.9213e-01,\n",
       "                        -5.0433e-01, -3.2120e+00,  7.3052e-01, -3.0903e-01,  7.2790e-01],\n",
       "                       [-1.8812e-01, -6.8485e-03,  8.5455e-01, -1.1377e+00, -2.6609e-01,\n",
       "                         1.9464e-01,  1.5561e-01,  7.6812e-02,  9.6909e-01, -7.0363e-01,\n",
       "                         6.5350e-01,  2.9669e-01, -1.0347e-01, -7.1086e-01,  3.1886e-01,\n",
       "                        -4.4680e-01, -3.4848e+00,  6.4793e-01, -3.4670e-01,  8.6266e-01],\n",
       "                       [-2.0480e-01,  1.1719e-02,  9.9975e-01, -1.1970e+00, -3.2021e-01,\n",
       "                         2.4933e-01,  3.0991e-01,  4.0540e-02,  8.6300e-01, -1.0280e+00,\n",
       "                         9.2947e-01,  2.6912e-01, -1.0681e-01, -6.7458e-01,  2.9797e-01,\n",
       "                        -3.4265e-01, -3.5959e+00,  6.6970e-01, -3.2481e-01,  1.0498e+00]],\n",
       "                      device='cuda:0'))])}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_community(\n",
    "    community,\n",
    "    *loaders,\n",
    "    optimizers=[optimizer, None],\n",
    "    config=get_training_dict(config),\n",
    "    show_all_acc=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"both\"\n",
    "decision_params = (\"last\", \"both\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57a15f98c430477f8de56a1430092005",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "cudnn RNN backward can only be called in training mode",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/gb21/Code/ANNs/community-of-agents/notebooks/Funcspec/Symbols.ipynb Cell 30\u001b[0m in \u001b[0;36m<cell line: 46>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bblossom.ee.ic.ac.uk/home/gb21/Code/ANNs/community-of-agents/notebooks/Funcspec/Symbols.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=75'>76</a>\u001b[0m loss, t_target, output \u001b[39m=\u001b[39m get_loss(output, t_target)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bblossom.ee.ic.ac.uk/home/gb21/Code/ANNs/community-of-agents/notebooks/Funcspec/Symbols.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=76'>77</a>\u001b[0m \u001b[39m#t_masks = [(target == t ).all(1) for t in target.unique(dim=0)]\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bblossom.ee.ic.ac.uk/home/gb21/Code/ANNs/community-of-agents/notebooks/Funcspec/Symbols.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=77'>78</a>\u001b[0m \u001b[39m#t_masks = [(target == t).any(1)]\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bblossom.ee.ic.ac.uk/home/gb21/Code/ANNs/community-of-agents/notebooks/Funcspec/Symbols.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=78'>79</a>\u001b[0m \u001b[39m#masks = [(t_target == 0)*t_masks[0], (t_target == 3)*t_masks[0]]\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bblossom.ee.ic.ac.uk/home/gb21/Code/ANNs/community-of-agents/notebooks/Funcspec/Symbols.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=79'>80</a>\u001b[0m \u001b[39m#factors = torch.ones(len(t_masks))\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bblossom.ee.ic.ac.uk/home/gb21/Code/ANNs/community-of-agents/notebooks/Funcspec/Symbols.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=80'>81</a>\u001b[0m \u001b[39m#factors[0] *= 1\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bblossom.ee.ic.ac.uk/home/gb21/Code/ANNs/community-of-agents/notebooks/Funcspec/Symbols.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=81'>82</a>\u001b[0m \u001b[39m#loss = torch.stack([loss[m].mean()*f for m, f in zip(t_masks, factors)]).mean()\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bblossom.ee.ic.ac.uk/home/gb21/Code/ANNs/community-of-agents/notebooks/Funcspec/Symbols.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=83'>84</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bblossom.ee.ic.ac.uk/home/gb21/Code/ANNs/community-of-agents/notebooks/Funcspec/Symbols.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=84'>85</a>\u001b[0m losses\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mitem())\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bblossom.ee.ic.ac.uk/home/gb21/Code/ANNs/community-of-agents/notebooks/Funcspec/Symbols.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=86'>87</a>\u001b[0m \u001b[39mif\u001b[39;00m check_gradients : \n",
      "File \u001b[0;32m~/.conda/envs/community/lib/python3.10/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/.conda/envs/community/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cudnn RNN backward can only be called in training mode"
     ]
    }
   ],
   "source": [
    "n_epochs = 50\n",
    "\n",
    "pbar = tqdm_n(range(n_epochs))\n",
    "descs = np.full((2), \"\", dtype=object)\n",
    "\n",
    "check_gradients = False\n",
    "train, test = True, True\n",
    "early_stop = True\n",
    "test_cheat = False\n",
    "\n",
    "force_connections = False\n",
    "\n",
    "best_acc = 0\n",
    "acc_hist, loss_hist = [[] for _ in range(2)], [[] for _ in range(2)]\n",
    "\n",
    "\n",
    "def get_loss(output, t_target):\n",
    "    n_target = len(t_target.shape)\n",
    "    n_decisions = output.shape[:-2]\n",
    "\n",
    "    if len(n_decisions) == 1:\n",
    "        n_decisions = n_decisions[0]\n",
    "    elif len(n_decisions) == 0:\n",
    "        n_decisions = 1\n",
    "\n",
    "    if n_target == n_decisions == 1:\n",
    "        loss = F.cross_entropy(output, t_target, reduction=\"none\")\n",
    "        output = output.unsqueeze(0)\n",
    "\n",
    "    elif n_target == 1 and n_decisions != 1:\n",
    "        t_target = t_target.unsqueeze(0).expand(output.shape[:-1])\n",
    "        loss = torch.stack(\n",
    "            [F.cross_entropy(o, t, reduction=\"none\") for o, t in zip(output, t_target)]\n",
    "        ).T\n",
    "\n",
    "    elif n_target != 1 and n_decisions == 1:\n",
    "        loss = torch.stack(\n",
    "            [F.cross_entropy(output, t, reduction=\"none\") for t in t_target]\n",
    "        ).T\n",
    "\n",
    "    elif n_target == n_decisions:\n",
    "        loss = torch.stack(\n",
    "            [F.cross_entropy(o, t, reduction=\"none\") for o, t in zip(output, t_target)]\n",
    "        ).T\n",
    "\n",
    "    else:\n",
    "        res = [get_loss(o, t_target) for o in output]\n",
    "        loss, t_target = torch.stack([r[0] for r in res]).T, torch.stack(\n",
    "            [r[1] for r in res]\n",
    "        )\n",
    "\n",
    "    return loss.mean(), t_target, output\n",
    "\n",
    "\n",
    "for epoch in pbar:\n",
    "    train_loader, test_loader = loaders\n",
    "\n",
    "    if train:\n",
    "        community.train()\n",
    "        # Training\n",
    "        losses, accs = [], []\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "            data = process_data(data, not use_conv, device)\n",
    "            t_target, target = get_task_target(target, task).to(device), target.to(\n",
    "                device\n",
    "            )\n",
    "\n",
    "            if test_cheat:\n",
    "                data[:, 0] = data[:, 1]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if force_connections:\n",
    "                conns = fconns[-1].detach().unsqueeze(0)\n",
    "                for ag in range(2):\n",
    "                    conns[:, ag, :, community.nonzero_received[ag]] = binary_conn(\n",
    "                        target, 1 - ag\n",
    "                    ).float()\n",
    "                conns[conns == 0] = -1\n",
    "            else:\n",
    "                conns = None\n",
    "\n",
    "            outputs, states, conns = community(data, conns)\n",
    "            # print((outputs[-1][0] == outputs[-1][1]).all())\n",
    "\n",
    "            output, deciding_ags = get_decision(outputs, *decision_params)\n",
    "\n",
    "            loss, t_target, output = get_loss(output, t_target)\n",
    "            # t_masks = [(target == t ).all(1) for t in target.unique(dim=0)]\n",
    "            # t_masks = [(target == t).any(1)]\n",
    "            # masks = [(t_target == 0)*t_masks[0], (t_target == 3)*t_masks[0]]\n",
    "            # factors = torch.ones(len(t_masks))\n",
    "            # factors[0] *= 1\n",
    "            # loss = torch.stack([loss[m].mean()*f for m, f in zip(t_masks, factors)]).mean()\n",
    "\n",
    "            loss.backward()\n",
    "            losses.append(loss.cpu().data.item())\n",
    "\n",
    "            if check_gradients:\n",
    "                zero_grads = np.array(\n",
    "                    [\n",
    "                        ((p.grad == 0).all()).cpu().data.item()\n",
    "                        for p in community.parameters()\n",
    "                        if p.grad is not None\n",
    "                    ]\n",
    "                )\n",
    "                none_grads = np.array([p.grad is None for p in community.parameters()])\n",
    "                zero_params = np.array(\n",
    "                    list(dict(community.named_parameters()).keys()), dtype=object\n",
    "                )[~none_grads][zero_grads]\n",
    "                none_params = np.array(\n",
    "                    list(dict(community.named_parameters()).keys()), dtype=object\n",
    "                )[none_grads]\n",
    "\n",
    "                print(f\"Zero params : {zero_params}\")\n",
    "                print(f\"None Params : {none_params}\")\n",
    "\n",
    "            optimizer.step()  # , scheduler.step()\n",
    "\n",
    "            pred = output.argmax(dim=-1)\n",
    "            correct = pred.eq(t_target.view_as(pred))\n",
    "            acc = (\n",
    "                (correct.sum(-1) * np.prod(t_target.shape[:-1]) / t_target.numel())\n",
    "                .cpu()\n",
    "                .data.numpy()\n",
    "            )\n",
    "\n",
    "            # acc = [(correct[m].sum()/m.sum()).cpu().data.numpy() for m in masks]\n",
    "\n",
    "            accs.append(acc)\n",
    "\n",
    "            descs[0] = str(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)] Loss: {:.3f}, Accuracy: {}%, Decider : {}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * batch_size,\n",
    "                    len(train_loader.dataset),\n",
    "                    100.0 * batch_idx / len(train_loader),\n",
    "                    loss.item(),\n",
    "                    ([np.round(100 * a) for a in acc])\n",
    "                    if type(acc) is list\n",
    "                    else np.round(100 * acc),\n",
    "                    np.round(deciding_ags.float().mean().cpu().data.item(), 1)\n",
    "                    if deciding_ags is not None\n",
    "                    else \"none\",\n",
    "                )\n",
    "            )\n",
    "\n",
    "            pbar.set_description((descs.sum()))\n",
    "\n",
    "        loss_hist[0].append(np.mean(losses))\n",
    "        acc_hist[0].append(np.mean(accs))\n",
    "\n",
    "    if test:\n",
    "        losses, accs = [], []\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "\n",
    "            data, target = process_data(data, not use_conv, device), target.to(device)\n",
    "            if test_cheat:\n",
    "                data[:, 0] = data[:, 1]\n",
    "            t_target = get_task_target(target, task).to(device)\n",
    "\n",
    "            if force_connections:\n",
    "                conns = fconns[-1].clone().detach().unsqueeze(0)\n",
    "                for ag in range(2):\n",
    "                    conns[:, ag, :, community.nonzero_received[ag]] = binary_conn(\n",
    "                        target, 1 - ag\n",
    "                    ).float()\n",
    "                conns[conns == 0] = -1\n",
    "            else:\n",
    "                conns = None\n",
    "\n",
    "            outputs, states, _ = community(data, conns)\n",
    "            # print((outputs[-1][0] == outputs[-1][1]).all())\n",
    "            output, deciding_ags = get_decision(outputs, *decision_params, t_target)\n",
    "\n",
    "            loss, t_target, output = get_loss(output, t_target)\n",
    "            loss = loss.mean()\n",
    "            losses.append(loss.cpu().data.item())\n",
    "\n",
    "            pred = output.argmax(dim=-1, keepdim=True)\n",
    "            correct = pred.eq(t_target.view_as(pred)).sum().cpu().data.item()\n",
    "            accs.append(correct / t_target.numel())\n",
    "\n",
    "        acc = np.mean(accs)\n",
    "\n",
    "        loss_hist[1].append(np.mean(losses))\n",
    "        acc_hist[1].append(np.mean(accs))\n",
    "\n",
    "        descs[1] = str(\n",
    "            \"| Test : Loss: {:.3f}, Accuracy: {}%\".format(\n",
    "                loss.item(),\n",
    "                (np.round(100 * a) for a in acc)\n",
    "                if type(acc) is list\n",
    "                else np.round(100 * acc),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        pbar.set_description((descs.sum()))\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    if acc > best_acc:\n",
    "        best_model = deepcopy(community)\n",
    "        best_acc = acc\n",
    "\n",
    "    if acc > 0.95 and early_stop:\n",
    "        break\n",
    "\n",
    "decision_params = (\"last\", \"both\")  # Change to '0', '1' or 'loss'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, constrained_layout=True, figsize=(12, 8))\n",
    "for metric, m_axs, m_name in zip([loss_hist, acc_hist], axs, [\"Loss\", \"Acc\"]):\n",
    "    for trial, ax, t_name in zip([0, 1], m_axs, [\"Train\", \"Test\"]):\n",
    "        ax.plot(metric[trial])\n",
    "        ax.set_title(f\"{t_name} {m_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(best_model.state_dict(), f'saves/network_{task}_{n_classes}{\"_non\"*(1 - data_config[\"static\"])}_static_p={sparsity}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_params = (\"last\", \"0\")\n",
    "\n",
    "\n",
    "def plot_accuracy_matrix(model, test_loader):\n",
    "\n",
    "    accs = []\n",
    "    targets, t_targets = [], []\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "\n",
    "        data, target = process_data(data, not use_conv, device), target.to(device)\n",
    "        t_target = get_task_target(target, task).to(device)\n",
    "\n",
    "        if force_connections:\n",
    "            conns = fconns.clone().detach()\n",
    "            for ag in range(2):\n",
    "                conns[:, ag, :, community.nonzero_received[ag]] = binary_conn(\n",
    "                    target, 1 - ag\n",
    "                )\n",
    "            conns[conns == 0] = -1\n",
    "        else:\n",
    "            conns = None\n",
    "\n",
    "        outputs, states, conns = model(data, conns)\n",
    "        # print((outputs[-1][0] == outputs[-1][1]).all())\n",
    "        output, deciding_ags = get_decision(outputs, *decision_params, target)\n",
    "\n",
    "        loss, t_target = get_loss(output, t_target, task)\n",
    "\n",
    "        pred = output.argmax(dim=-1, keepdim=True)\n",
    "        correct = pred.eq(t_target.view_as(pred)).cpu().data\n",
    "        targets.append(target.cpu())\n",
    "        t_targets.append(t_target.cpu())\n",
    "        accs.append(correct)\n",
    "\n",
    "    accs, targets = torch.cat(accs), torch.cat(targets)\n",
    "    n_classes = len(targets.unique())\n",
    "    t_masks = [(targets == t).all(1) for t in targets.unique(dim=0)]\n",
    "    acc_per_target = np.array([accs[m].float().mean() for m in t_masks]).reshape(\n",
    "        n_classes, n_classes\n",
    "    )\n",
    "\n",
    "    # acc_per_target = np.array([[acc_per_target[t1*n_classes + t2].cpu().data.item() for t1 in range(n_classes)] for t2 in range(n_classes)])\n",
    "\n",
    "    ax = sns.heatmap(\n",
    "        acc_per_target, cmap=\"inferno\", annot=None, annot_kws={\"fontsize\": 10}, fmt=\"s\"\n",
    "    )\n",
    "    ax.set_title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_accuracy_matrix(best_model, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_mat(model, test_loader):\n",
    "\n",
    "    accs, preds = [], []\n",
    "    targets, t_targets = [], []\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "\n",
    "        data, target = process_data(data, not use_conv, device), target.to(device)\n",
    "        t_target = get_task_target(target, task).to(device)\n",
    "\n",
    "        outputs, states, conns = model(data)\n",
    "        # print((outputs[-1][0] == outputs[-1][1]).all())\n",
    "        output, deciding_ags = get_decision(outputs, *decision_params, target)\n",
    "\n",
    "        loss, t_target = get_loss(output, t_target, task)\n",
    "        pred = output.argmax(dim=-1, keepdim=True)\n",
    "\n",
    "        correct = pred.eq(t_target.view_as(pred)).cpu().data\n",
    "        targets.append(target.cpu())\n",
    "        t_targets.append(t_target.cpu())\n",
    "        accs.append(correct)\n",
    "        preds.append(pred.cpu())\n",
    "\n",
    "    preds, t_targets = torch.cat(preds), torch.cat(t_targets)\n",
    "\n",
    "    n_classes = len(t_targets.unique())\n",
    "    t_masks = [(t_targets == t) for t in t_targets.unique()]\n",
    "\n",
    "    pred_per_target = [preds[m] for m in t_masks]\n",
    "    n_pred_per_target = np.zeros((n_classes, n_classes))\n",
    "    for t, p in enumerate(pred_per_target):\n",
    "        for (u, c) in zip(*p.unique(return_counts=True)):\n",
    "            n_pred_per_target[t, u] = c\n",
    "\n",
    "    n_pred_per_target /= n_pred_per_target.sum(1)\n",
    "\n",
    "    # acc_per_target = np.array([[acc_per_target[t1*n_classes + t2].cpu().data.item() for t1 in range(n_classes)] for t2 in range(n_classes)])\n",
    "\n",
    "    ax = sns.heatmap(\n",
    "        n_pred_per_target, cmap=\"inferno\", annot_kws={\"fontsize\": 10}, fmt=\"s\"\n",
    "    )\n",
    "    ax.set_title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "    return n_pred_per_target\n",
    "\n",
    "\n",
    "n_pred_per_target = plot_confusion_mat(best_model, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dfe8de",
   "metadata": {},
   "source": [
    "### Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7ecd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "community = best_model\n",
    "community.to(device)\n",
    "nonzero_idxs = community.nonzero_received\n",
    "n_conns = len(nonzero_idxs[0])\n",
    "\n",
    "# for data, target in loaders[1] :\n",
    "data, target = get_data(flatten=not use_conv)\n",
    "\n",
    "# data, target = process_data(data, True, True, device), target.to(device)\n",
    "out, states, conns = best_model(data.to(device))\n",
    "\n",
    "if force_connections:\n",
    "    conns = -torch.ones_like(conns).to(device)\n",
    "    for ag in range(2):\n",
    "        conns[:, ag, :, community.nonzero_received[ag][:n_bits]] = binary_conn(\n",
    "            target, 1 - ag, n_classes\n",
    "        )\n",
    "\n",
    "    conns[conns == 0] = -1\n",
    "\n",
    "# conns[-1][0].count_nonzero(dim=0)\n",
    "# torch.stack([conns[-1][i].count_nonzero(dim=0).max() for i in range(2)])\n",
    "# sums.append(torch.tensor([[conns[-1][i][target[:, i] == t].sum() for t in range(4)] for i in range(2)]))\n",
    "\n",
    "\"\"\"\n",
    "ncols = int(np.sqrt(n_conns))\n",
    "nrows = n_conns // ncols\n",
    "if ncols * nrows < n_conns : \n",
    "    ncols += 1\n",
    "print(nrows, ncols)\n",
    "\"\"\"\n",
    "\n",
    "if data_config[\"static\"]:\n",
    "\n",
    "    sums = torch.tensor(\n",
    "        [\n",
    "            [\n",
    "                conns[-1, ag, target[:, 1 - ag] == t][:, nonzero_idxs[ag]].sum(0)\n",
    "                for t in range(n_classes)\n",
    "            ]\n",
    "            for ag in range(2)\n",
    "        ]\n",
    "    )\n",
    "    if n_conns == 1:\n",
    "        sums = sums.unsqueeze(-1)\n",
    "\n",
    "    if n_classes < 4:\n",
    "\n",
    "        nrows, ncols = 2, n_classes\n",
    "\n",
    "        fig, axs = plt.subplots(\n",
    "            nrows,\n",
    "            ncols,\n",
    "            figsize=(n_classes * (n_conns), 2),\n",
    "            constrained_layout=True,\n",
    "            dpi=100,\n",
    "            sharey=True,\n",
    "        )\n",
    "\n",
    "        for ag, ag_axs in enumerate(axs):\n",
    "            for dig, ax in enumerate(ag_axs):\n",
    "                sum = sums[ag][dig].cpu().data.numpy()\n",
    "                bars = ax.bar(np.arange(len(sum)), sum, color=col)\n",
    "                # ax.set_xticks(np.arange(len(sum)))\n",
    "                # ax.bar_label(bars)\n",
    "                # sns.heatmap(sum, cmap=\"inferno\", annot=sum.round(1).astype(str), annot_kws={'fontsize': 16}, fmt='s', ax=ax)\n",
    "\n",
    "                ax.set_xticks([])\n",
    "                ax.set_xlabel(f\"Digit {dig}\")\n",
    "                if dig == 0:\n",
    "                    ax.set_ylabel(f\"Ag {ag}\")\n",
    "    else:\n",
    "        lines = [\"-\", \"--\"]\n",
    "        fig = plt.figure(constrained_layout=True, figsize=(12, 4), dpi=130)\n",
    "        fig.suptitle(\"Connections Per Classes\")\n",
    "        subfigs = fig.subfigures(nrows=2, ncols=1)\n",
    "        nrows, ncols = 1, 1\n",
    "\n",
    "        for ag, subfig in enumerate(subfigs):\n",
    "            subfig.suptitle(f\"Agent {ag}\")\n",
    "            sum = sums[ag]\n",
    "            ax = subfig.subplots(nrows=nrows, ncols=ncols)\n",
    "\n",
    "            [\n",
    "                ax.plot(\n",
    "                    range(len(s)),\n",
    "                    s.cpu().data.numpy(),\n",
    "                    linestyle=lines[c % 2],\n",
    "                    label=f\"Connection {c}\",\n",
    "                )\n",
    "                for c, s in enumerate(sum.T)\n",
    "            ]\n",
    "            ax.set_xlabel(\"Classes\")\n",
    "            ax.legend()\n",
    "            # ax.set_title(f'Agent {i}')\n",
    "\n",
    "else:\n",
    "    nrows, ncols = 1, 1\n",
    "\n",
    "    lines = [\"-\", \"--\"]\n",
    "    fig = plt.figure(constrained_layout=True, figsize=(12, 4), dpi=130)\n",
    "    fig.suptitle(\"Connections Through Time\")\n",
    "    subfigs = fig.subfigures(nrows=2, ncols=1)\n",
    "\n",
    "    sums = torch.stack(\n",
    "        [\n",
    "            torch.stack(\n",
    "                [\n",
    "                    community.nonzero_receivedonns[:, i, target[:, 1 - i] == t][\n",
    "                        ..., nonzero_idxs[1 - i]\n",
    "                    ].sum(1)\n",
    "                    for t in range(n_classes)\n",
    "                ]\n",
    "            )\n",
    "            for i in range(2)\n",
    "        ]\n",
    "    )\n",
    "    for ag, subfig in enumerate(subfigs):\n",
    "        subfig.suptitle(f\"Agent {ag}\")\n",
    "\n",
    "        axs = subfig.subplots(nrows=nrows, ncols=ncols)\n",
    "        if n_conns == 1:\n",
    "            axs = np.array([axs])\n",
    "        for i, ax in enumerate(axs.flatten()):\n",
    "            sum = sums.cpu().data.numpy()[ag, ..., i]\n",
    "            for t, s in enumerate(sum):\n",
    "                ax.plot(range(len(s)), s, label=f\"Digit {t}\", linestyle=lines[t % 2])\n",
    "\n",
    "            ax.legend()\n",
    "            ax.set_title(f\"Connection {i}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, target = next(iter(loaders[1]))\n",
    "plot_data = data[:5, -1, :].reshape(-1, 1, 100, 50).cpu().data\n",
    "t_target = get_task_target(target, task)\n",
    "plot_data[..., 50, :] = 0.5\n",
    "plot_grid(\n",
    "    plot_data,\n",
    "    [\n",
    "        f\"Counts : {t.data.numpy()} \\n Target : {t_t.item()}\"\n",
    "        for t, t_t in zip(target, t_target)\n",
    "    ],\n",
    "    figsize=(10, 10),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2318caa3",
   "metadata": {},
   "source": [
    "### Decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4859de41",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_all_targets = lambda: torch.cat([t for _, t in loaders[1]])\n",
    "\n",
    "all_targets = get_all_targets()\n",
    "\n",
    "uniques = all_targets.unique(dim=0).cpu()\n",
    "decision_means = torch.zeros(len(uniques))\n",
    "\n",
    "community.to(\"cpu\")\n",
    "\n",
    "for b_idx, (data, target) in enumerate(loaders[1]):\n",
    "\n",
    "    data, target = process_data(data, True, True, \"cpu\"), target.to(\"cpu\")\n",
    "    t_target = get_task_target(target, task).cpu()\n",
    "    output, *_ = community(data)\n",
    "    output, decision_ags = get_decision(output, decision_params, target=t_target)\n",
    "\n",
    "    for i, t in enumerate(uniques):\n",
    "        mask = target.eq(torch.tensor(t)).all(axis=1)\n",
    "        if mask.sum() != 0:\n",
    "            decision_means[i] += decision_ags[mask].float().cpu().sum() / mask.sum()\n",
    "\n",
    "decision_means /= b_idx + 1\n",
    "\n",
    "digits_in = lambda d1, d2: (torch.tensor([d1, d2]) == uniques).all(1).any()\n",
    "digits_idx = lambda d1, d2: (torch.tensor([d1, d2]) == uniques).all(1).float().argmax()\n",
    "decisions = np.zeros((n_classes, n_classes))\n",
    "targets = np.zeros((n_classes, n_classes), dtype=object)\n",
    "\n",
    "for d1 in range(n_classes):\n",
    "    for d2 in range(n_classes):\n",
    "        if digits_in(d1, d2):\n",
    "            decisions[d1, d2] = decision_means[digits_idx(d1, d2)]\n",
    "            targets[d1, d2] = str(\n",
    "                get_task_target(uniques, task)[digits_idx(d1, d2)].cpu().data.item()\n",
    "            )\n",
    "        else:\n",
    "            decisions[d1, d2] = -0.1\n",
    "            targets[d1, d2] = \"X\"\n",
    "\n",
    "ax = sns.heatmap(\n",
    "    decisions, cmap=\"inferno\", annot=targets, annot_kws={\"fontsize\": 16}, fmt=\"s\"\n",
    ")\n",
    "ax.set_title(\"Average decison-making agent for all targets\")\n",
    "\n",
    "ax.set_xlabel(\"Digit received by Agent 1\")\n",
    "ax.set_ylabel(\"Digit received by Agent 0\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "community.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "if use_conv:\n",
    "\n",
    "    convs = [\n",
    "        [c for c in ag.conv if type(c) is torch.nn.Conv2d] for ag in community.agents\n",
    "    ]\n",
    "    symbols = datasets[0].symbols[:-1]\n",
    "\n",
    "    corrs = np.array(\n",
    "        [\n",
    "            [\n",
    "                [\n",
    "                    pearsonr(kernel.cpu().data.numpy().flatten(), symbol.flatten())[0]\n",
    "                    for kernel in conv[0].weight[:, 0]\n",
    "                ]\n",
    "                for conv in convs\n",
    "            ]\n",
    "            for symbol in symbols\n",
    "        ]\n",
    "    )  # 2 symbols x 2 convs x n_channels_out\n",
    "\n",
    "    fig = plt.figure(\n",
    "        constrained_layout=True, figsize=((convs[0][0].out_channels + 1) * 2, 5)\n",
    "    )\n",
    "    # fig.suptitle('Conv Weights')\n",
    "\n",
    "    # create 3x1 subfigs\n",
    "    subfigs = fig.subfigures(nrows=len(convs) + 1, ncols=1)\n",
    "    for row, (subfig, conv) in enumerate(zip(subfigs, convs)):\n",
    "        subfig.suptitle(f\"Agent {row}\")\n",
    "\n",
    "        # create 1x3 subplots per subfig\n",
    "        axs = subfig.subplots(nrows=len(conv), ncols=conv[0].out_channels)\n",
    "\n",
    "        for col, ax in enumerate(axs.flatten()):\n",
    "            im = ax.imshow((conv[0].weight.data.cpu().numpy()[col, 0]))\n",
    "            ax.set_title(f\"Conv weight {col} \\n Corr = {corrs[:, row, col].round(1)}\")\n",
    "\n",
    "        cbar = subfig.colorbar(im, ax=axs.ravel().tolist(), shrink=0.95)\n",
    "\n",
    "    axs = subfigs[-1].subplots(nrows=1, ncols=2)\n",
    "    for a, (ax, symb) in enumerate(zip(axs, symbols)):\n",
    "        im = ax.imshow(symb)\n",
    "        ax.set_title(f\"Mean corr to symbol per ag : {corrs[a].mean(-1).round(2)}\")\n",
    "    subfigs[-1].suptitle(\"Symbols to be detected\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_max = lambda c: (c.max() - c.min()) / c.sum()\n",
    "[diff_max(corr / corr.max()) for corr in corrs.mean(-1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(product(*(convs, symbols))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funcspec Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from community.funcspec.correlation import (\n",
    "    v_pearsonr,\n",
    "    get_correlation,\n",
    "    randperm_no_fixed,\n",
    "    get_pearson_metrics,\n",
    "    fixed_information_data,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_metric = lambda c: (c[0] - c[1]) / c.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b89098301147468f977e735bb2299286",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Correlation Metric Trials:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corrs_metrics = get_pearson_metrics(\n",
    "    community, loaders, symbols=True, use_tqdm=True, device=device, double_data=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 2, 3), (2, 2, 3), (2, 3)]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[c.shape for c in corrs_metrics.values()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs  # agents x timesteps x target\n",
    "fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "for ag, ag_axs in enumerate(axs):\n",
    "    ag_corrs = corrs.mean(-1).mean(-1)[:, ag]\n",
    "    for t in range(2):\n",
    "        t_corrs = ag_corrs[t]\n",
    "        ag_axs[0].bar(range(len(t_corrs)), (-1) ** t * t_corrs, label=f\"Digit {t}\")\n",
    "\n",
    "    diff_corrs = [diff_max(c) for c in ag_corrs.T]\n",
    "    ag_axs[1].bar(range(len(diff_corrs)), diff_corrs)\n",
    "\n",
    "    ag_axs[0].set_title(f\"Correlations for Agent {ag}\")\n",
    "    for ax in ag_axs:\n",
    "        ax.set_xlabel(\"Timesteps\")\n",
    "        ax.set_xticks(range(len(t_corrs)))\n",
    "        ax.set_xticklabels([\"Before Comms\", \"After Comms\"])\n",
    "        ax.legend()\n",
    "    ag_axs[0].set_ylabel(\"Correlation\")\n",
    "    ag_axs[1].set_title(f\"Correlation Diff for Agent {ag}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "for ag, ag_axs in enumerate(axs):\n",
    "    ag_corrs = corrs.mean(-1).mean(-1)[:, ag]\n",
    "    for t in range(2):\n",
    "        t_corrs = ag_corrs[t] / base_corrs.mean(-1)[ag]\n",
    "        ag_axs[0].bar(range(len(t_corrs)), (-1) ** t * t_corrs, label=f\"Digit {t}\")\n",
    "\n",
    "    diff_corrs = [diff_max(c) for c in ag_corrs.T]\n",
    "    ag_axs[1].bar(range(len(diff_corrs)), diff_corrs)\n",
    "\n",
    "    ag_axs[0].set_title(f\"Correlations for Agent {ag}\")\n",
    "    for ax in ag_axs:\n",
    "        ax.set_xlabel(\"Timesteps\")\n",
    "        ax.set_xticks(range(len(t_corrs)))\n",
    "        ax.set_xticklabels([\"Before Comms\", \"After Comms\"])\n",
    "        ax.legend()\n",
    "    ag_axs[0].set_ylabel(\"Correlation\")\n",
    "    ag_axs[0].legend()\n",
    "    ag_axs[1].set_title(f\"Correlation Diff for Agent {ag}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bottleneck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c75f902b20d14995b6162b391cebc23d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Bottleneck Metric Trials :   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48018d1ef25844e1a5dd1965b52bc05b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch::   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58c07fe73f6343339b40da0622e64821",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Epoch::   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from community.funcspec.bottleneck import readout_retrain\n",
    "\n",
    "bottleneck_metrics = readout_retrain(\n",
    "    community,\n",
    "    loaders,\n",
    "    n_classes,\n",
    "    n_epochs=1,\n",
    "    n_tests=1,\n",
    "    use_tqdm=True,\n",
    "    symbols=True,\n",
    "    force_connections=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03866186, 0.08012821], dtype=float32)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bottleneck_metrics[\"accs\"][1, :, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-0.083508775, 0.14965491], [-0.34907252, 0.10476188]]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\n",
    "    [diff_metric(bottleneck_metrics[\"accs\"][ag, :, step]) for step in range(2)]\n",
    "    for ag in range(2)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    \"base_corrs\": corrs_metrics[\"base_corrs\"],\n",
    "    \"mean_corrs\": corrs_metrics[\"mean_corrs\"],\n",
    "    \"bottleneck\": bottleneck_metrics[\"accs\"].mean(-1),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from community.funcspec.single_model_loop import define_and_log\n",
    "\n",
    "table, table_diff = define_and_log(list(metrics.values()), list(metrics.keys()), 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.13.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.15"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/gb21/Code/ANNs/community-of-agents/notebooks/Funcspec/wandb/run-20220929_124931-3tjb23qo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/gbena/funcspec/runs/3tjb23qo\" target=\"_blank\">rare-dust-783</a></strong> to <a href=\"https://wandb.ai/gbena/funcspec\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "423eccaabd8c4236b14edcc48e277757",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.005 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.924328…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">rare-dust-783</strong>: <a href=\"https://wandb.ai/gbena/funcspec/runs/3tjb23qo\" target=\"_blank\">https://wandb.ai/gbena/funcspec/runs/3tjb23qo</a><br/>Synced 5 W&B file(s), 2 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220929_124931-3tjb23qo/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "\n",
    "wandb.init(project=\"funcspec\", entity=\"gbena\", config={})\n",
    "\n",
    "wandb.log({\"Metric Results\": table})\n",
    "wandb.log({\"Metric Results Diff\": table_diff})\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'diff_max' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/gb21/Code/ANNs/community-of-agents/notebooks/Funcspec/Symbols.ipynb Cell 64\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bblossom.ee.ic.ac.uk/home/gb21/Code/ANNs/community-of-agents/notebooks/Funcspec/Symbols.ipynb#Y122sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     t_accs \u001b[39m=\u001b[39m  ag_accs[t]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bblossom.ee.ic.ac.uk/home/gb21/Code/ANNs/community-of-agents/notebooks/Funcspec/Symbols.ipynb#Y122sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     ag_axs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mbar(\u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(t_accs)), (\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mt \u001b[39m*\u001b[39m t_accs, label\u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mDigit \u001b[39m\u001b[39m{\u001b[39;00mt\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bblossom.ee.ic.ac.uk/home/gb21/Code/ANNs/community-of-agents/notebooks/Funcspec/Symbols.ipynb#Y122sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m diff_corrs \u001b[39m=\u001b[39m [diff_max(c) \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m ag_accs\u001b[39m.\u001b[39mT]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bblossom.ee.ic.ac.uk/home/gb21/Code/ANNs/community-of-agents/notebooks/Funcspec/Symbols.ipynb#Y122sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m ag_axs[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mbar(\u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(diff_corrs)), diff_corrs)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bblossom.ee.ic.ac.uk/home/gb21/Code/ANNs/community-of-agents/notebooks/Funcspec/Symbols.ipynb#Y122sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m ag_axs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mset_title(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mRetrain Accs for Agent \u001b[39m\u001b[39m{\u001b[39;00mag\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32m/home/gb21/Code/ANNs/community-of-agents/notebooks/Funcspec/Symbols.ipynb Cell 64\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bblossom.ee.ic.ac.uk/home/gb21/Code/ANNs/community-of-agents/notebooks/Funcspec/Symbols.ipynb#Y122sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     t_accs \u001b[39m=\u001b[39m  ag_accs[t]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bblossom.ee.ic.ac.uk/home/gb21/Code/ANNs/community-of-agents/notebooks/Funcspec/Symbols.ipynb#Y122sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     ag_axs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mbar(\u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(t_accs)), (\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mt \u001b[39m*\u001b[39m t_accs, label\u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mDigit \u001b[39m\u001b[39m{\u001b[39;00mt\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bblossom.ee.ic.ac.uk/home/gb21/Code/ANNs/community-of-agents/notebooks/Funcspec/Symbols.ipynb#Y122sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m diff_corrs \u001b[39m=\u001b[39m [diff_max(c) \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m ag_accs\u001b[39m.\u001b[39mT]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bblossom.ee.ic.ac.uk/home/gb21/Code/ANNs/community-of-agents/notebooks/Funcspec/Symbols.ipynb#Y122sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m ag_axs[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mbar(\u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(diff_corrs)), diff_corrs)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bblossom.ee.ic.ac.uk/home/gb21/Code/ANNs/community-of-agents/notebooks/Funcspec/Symbols.ipynb#Y122sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m ag_axs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mset_title(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mRetrain Accs for Agent \u001b[39m\u001b[39m{\u001b[39;00mag\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'diff_max' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x720 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_accs = bottleneck_metrics[\"accs\"][0]  # target x timesteps x agents\n",
    "fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "for ag, ag_axs in enumerate(axs):\n",
    "    ag_accs = all_accs[..., ag]\n",
    "    for t in range(2):\n",
    "        t_accs = ag_accs[t]\n",
    "        ag_axs[0].bar(range(len(t_accs)), (-1) ** t * t_accs, label=f\"Digit {t}\")\n",
    "\n",
    "    diff_corrs = [diff_max(c) for c in ag_accs.T]\n",
    "    ag_axs[1].bar(range(len(diff_corrs)), diff_corrs)\n",
    "\n",
    "    ag_axs[0].set_title(f\"Retrain Accs for Agent {ag}\")\n",
    "    ag_axs[0].set_ylabel(\"Acc\")\n",
    "    for ax in ag_axs:\n",
    "        ax.set_xlabel(\"Timesteps\")\n",
    "        ax.set_xticks(range(len(t_accs)))\n",
    "        ax.set_xticklabels([\"Before Comms\", \"After Comms\"])\n",
    "        ax.legend()\n",
    "\n",
    "    ag_axs[1].set_title(f\"Retrain Acc Diff for Agent {ag}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ag_accs[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Correlation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_variations(dataset, index):\n",
    "\n",
    "    original_data, original_label = (d.clone() for d in dataset[index])\n",
    "\n",
    "    scenarios = [\"Original\", \"Same_Count\", \"0s\", \"1s\", \"Inv\"]\n",
    "    labels = [\n",
    "        [original_label],\n",
    "        [original_label],\n",
    "        [torch.tensor([n_classes - 1, 0])],\n",
    "        [torch.tensor([0, n_classes - 1])],\n",
    "        [original_label],\n",
    "    ]\n",
    "    labels = [l * 10 for l in labels]\n",
    "\n",
    "    data_var = {s: [] for s in scenarios}\n",
    "    for l in labels[0]:\n",
    "        data_var[\"Original\"].append(original_data)\n",
    "\n",
    "    dataset.regenerate = True\n",
    "    for i, (lab, s) in enumerate(zip(labels[1:], scenarios[1:])):\n",
    "\n",
    "        def regen_data(l, s):\n",
    "            dataset.data[1][index] = l\n",
    "            return dataset.__getitem__(index, inv=(\"Inv\" in s))[0]\n",
    "\n",
    "        for l in lab:\n",
    "            data_var[s].append(regen_data(l, s))\n",
    "\n",
    "    dataset.regenerate = False\n",
    "    dataset.data[1][index] = original_label\n",
    "    return {s: torch.stack(d).cpu() for s, d in data_var.items()}, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize data variations :\n",
    "datas, labels = get_input_variations(datasets[0], 5)\n",
    "datas = {s: process_data(data, flatten=False) for s, data in datas.items()}\n",
    "labels = torch.stack([l for lab in labels for l in lab[:2] if type(lab) is list])\n",
    "create_gifs(\n",
    "    torch.cat([d[:, :, :2, 0, ...] for d in datas.values()], 2),\n",
    "    labels,\n",
    "    \"symbols\",\n",
    "    data_config[\"input_size\"],\n",
    "    \"max_count\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "from community.funcspec.correlation import (\n",
    "    v_pearsonr,\n",
    "    get_correlation,\n",
    "    randperm_no_fixed,\n",
    ")\n",
    "\n",
    "perm = lambda s: randperm_no_fixed(s.shape[1])\n",
    "\n",
    "\n",
    "def vect_pearson_diff(h1, h2):\n",
    "    # Returns vectorized pearson correlation for nonequal vectors\n",
    "    mask = (h1 != h2).all(1)\n",
    "    corrs = (\n",
    "        v_pearsonr(h1[mask], h2[mask])[0] if mask.sum() > 0 else v_pearsonr(h1, h2)[0]\n",
    "    )\n",
    "    return corrs.mean()\n",
    "\n",
    "\n",
    "def get_correlations(network, dataset, n_tests=32):\n",
    "\n",
    "    corrs = []\n",
    "\n",
    "    for i in trange(min(n_tests, len(dataset))):\n",
    "\n",
    "        data_dict = get_input_variations(datasets[0], i)[0]\n",
    "        data_dict = {\n",
    "            s: [process_data(d, flatten=not use_conv, device=device)]\n",
    "            for s, d in data_dict.items()\n",
    "        }\n",
    "        # split = int(max(np.floor(data_dict['Original'].shape[2] / batch_size), 1))\n",
    "        # print(f'{data_dict[\"Original\"].shape[2]} input to process, splitting in {split}  batches')\n",
    "        # data_dict = {s : d.split(split, dim=2) for s, d in data_dict.items()}\n",
    "\n",
    "        states = {\n",
    "            s: np.concatenate(\n",
    "                [network(d)[1][-1].clone().cpu().data.numpy() for d in data], 1\n",
    "            )\n",
    "            for s, data in data_dict.items()\n",
    "        }\n",
    "        corrs.append(\n",
    "            np.array(\n",
    "                [\n",
    "                    [\n",
    "                        [vect_pearson_diff(h1[i], h2[i]) for h1 in states.values()]\n",
    "                        for h2 in states.values()\n",
    "                    ]\n",
    "                    for i in range(2)\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return np.stack(corrs), states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs, states = get_correlations(community, datasets[0], 256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1, figsize=(4, 7), dpi=120)\n",
    "scenarios = list(states.keys())\n",
    "\n",
    "for i, (ax, c) in enumerate(zip(axs, corrs.mean(0))):\n",
    "    ax = sns.heatmap(\n",
    "        c,\n",
    "        cmap=\"inferno\",\n",
    "        annot=c.round(3).astype(str),\n",
    "        annot_kws={\"fontsize\": 7},\n",
    "        fmt=\"s\",\n",
    "        ax=ax,\n",
    "    )\n",
    "    ax.set_title(f\"Correlations for Ag {i}\")\n",
    "    ax.set_xticks(np.arange(len(scenarios)) + 0.5)\n",
    "    ax.set_xticklabels(scenarios, fontsize=7)\n",
    "    ax.set_yticks(np.arange(len(scenarios)) + 0.5)\n",
    "    ax.set_yticklabels(scenarios, fontsize=7)\n",
    "\n",
    "# fig.colorbar(im)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_same_count_data(dataset, idx):\n",
    "\n",
    "    dataset.regenerate = True\n",
    "    orig_data, orig_label = dataset[idx]\n",
    "    sym_assigns = [dataset.symbol_assignments[l] for l in orig_label]\n",
    "    # print(sym_assigns)\n",
    "\n",
    "    same_count_datas = torch.stack(\n",
    "        [\n",
    "            dataset.__getitem__(idx, symbol_assigns=[s1, s2])[0]\n",
    "            for s1 in sym_assigns[0]\n",
    "            for s2 in sym_assigns[1]\n",
    "        ]\n",
    "    )\n",
    "    same_count_datas_0 = torch.stack(\n",
    "        [\n",
    "            dataset.__getitem__(idx, symbol_assigns=[s1, None])[0]\n",
    "            for s1 in sym_assigns[0]\n",
    "        ]\n",
    "    )\n",
    "    same_count_datas_1 = torch.stack(\n",
    "        [\n",
    "            dataset.__getitem__(idx, symbol_assigns=[None, s2])[0]\n",
    "            for s2 in sym_assigns[1]\n",
    "        ]\n",
    "    )\n",
    "    # same_count_datas = torch.stack([ torch.stack((d1[:, 0], d2[:, 1])) for d1 in same_count_datas_0 for d2 in same_count_datas_1])\n",
    "\n",
    "    dataset.regenerate = False\n",
    "\n",
    "    return same_count_datas, [same_count_datas_0, same_count_datas_1], orig_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 10\n",
    "dataset = datasets[0]\n",
    "data, target = dataset[idx]\n",
    "same_count_datas, same_count_datas_per_ag, original_data = get_same_count_data(\n",
    "    dataset, idx\n",
    ")\n",
    "\n",
    "fig = plt.figure(constrained_layout=True, figsize=(5, 5))\n",
    "fig.suptitle(\"Same Count Data\")\n",
    "\n",
    "nrows, ncols = same_count_datas.shape[:2]\n",
    "\n",
    "subfigs = fig.subfigures(nrows=2, ncols=1)\n",
    "\n",
    "for ag, subfig in enumerate(subfigs):\n",
    "    data = same_count_datas_per_ag[ag][:5]\n",
    "    subfig.suptitle(f\"Agent {ag} : Count {target[ag]}\")\n",
    "    axs = subfig.subplots(nrows=1, ncols=len(data), sharey=False)\n",
    "    if len(data) == 1:\n",
    "        axs = np.array([axs])\n",
    "    for p, ax in enumerate(axs.flatten()):\n",
    "        ax.imshow(data[p, -1, ag])\n",
    "        # ax.set_title(f'Agent {p}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_count_states = [\n",
    "    community(process_data(d, device=device))[1][-1].cpu().data.numpy()\n",
    "    for d in same_count_datas_per_ag\n",
    "]\n",
    "same_count_corrs = np.array(\n",
    "    [\n",
    "        [pearsonr(h1, h2)[0] for h1 in states[ag] for h2 in states[ag]]\n",
    "        for ag, states in enumerate(same_count_states)\n",
    "    ]\n",
    ").reshape(-1, 3, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_count_states = (\n",
    "    community(process_data(same_count_datas, device=device))[1][-1].cpu().data.numpy()\n",
    ")\n",
    "same_count_corrs = np.array(\n",
    "    [\n",
    "        [pearsonr(h1, h2)[0] for h1 in states for h2 in states]\n",
    "        for ag, states in enumerate(same_count_states)\n",
    "    ]\n",
    ").reshape(-1, 9, 9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_count_states.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for corr, data in zip(same_count_corrs, same_count_corrs):\n",
    "    plt.figure()\n",
    "    plt.imshow(corr)\n",
    "    plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from community.funcspec.single_model_loop import compute_all_metrics\n",
    "from community.funcspec.correlation import get_pearson_metrics, fixed_information_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 8\n",
    "dataset = datasets[0]\n",
    "data, target = dataset[idx]\n",
    "same_count_datas = fixed_information_data(data, target, 0)\n",
    "\n",
    "fig = plt.figure(constrained_layout=True, figsize=(5, 5))\n",
    "fig.suptitle(\"Same Count Data\")\n",
    "\n",
    "nrows, ncols = same_count_datas.shape[:2]\n",
    "\n",
    "subfigs = fig.subfigures(nrows=2, ncols=1)\n",
    "\n",
    "for ag, subfig in enumerate(subfigs):\n",
    "    data = same_count_datas_per_ag[ag]\n",
    "    subfig.suptitle(f\"Agent {ag} : Count {target[ag]}\")\n",
    "    axs = subfig.subplots(nrows=1, ncols=len(data), sharey=False)\n",
    "    if len(data) == 1:\n",
    "        axs = np.array([axs])\n",
    "    for p, ax in enumerate(axs.flatten()):\n",
    "        ax.imshow(data[p, -1, ag])\n",
    "        # ax.set_title(f'Agent {p}')\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, target = next(iter(loaders[0]))\n",
    "data = process_data(data, False)\n",
    "same_count_datas = fixed_information_data(data, target, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(constrained_layout=True, figsize=(15, 15))\n",
    "fig.suptitle(\"Same Count Data\")\n",
    "\n",
    "subfigs = fig.subfigures(nrows=len(same_count_datas), ncols=1)\n",
    "\n",
    "for d, subfig in enumerate(subfigs):\n",
    "    data = same_count_datas[d]\n",
    "    subfig.suptitle(f\"Item {d} : Count {target[d, 0]}\")\n",
    "    axs = subfig.subplots(nrows=2, ncols=5, sharey=False)\n",
    "    if len(data) == 1:\n",
    "        axs = np.array([axs])\n",
    "    for ag, ag_ax in enumerate(axs):\n",
    "        for p, ax in enumerate(ag_ax.flatten()):\n",
    "            ax.imshow(data[-1, ag, p, 0].cpu())\n",
    "        # ax.set_title(f'Agent {p}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('community')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5c46eabf39b4d4e6cb6668853226ee702b3f0cb279968f228c052b13b97983d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
