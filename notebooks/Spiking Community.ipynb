{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5278504c-3560-4e65-8424-e3366938a721",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Society of Mind using Spiking Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01bc2bb9-a5da-4cc6-b147-3f504bf5232b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import numpy.random as rd\n",
    "import time\n",
    "import argparse\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import importlib\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import copy\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from tqdm import tqdm\n",
    "#from tqdm.notebook import tqdm, trange\n",
    "\n",
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75e59ebd-2570-425e-8fd9-33c81feaf17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import *\n",
    "from decision import *\n",
    "from training import *\n",
    "from utils_diverse import *\n",
    "from spiking_models import *\n",
    "from datasets_and_tasks import*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "931fc019-df93-47b3-960f-de8569264ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/home/gb21/Code/SNNs/randman/\")\n",
    "from randman_snn import *\n",
    "sys.path.append('/home/gb21/Code/SNNs/Recurrent SNNs/')\n",
    "import rec_Layer\n",
    "from rec_Layer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40329de6-67e0-4d05-a507-53d84c3eb69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DCLL_path = '/home/gb21/Code/SNNs/DCLL/decolle-public/'\n",
    "sys.path.append(DCLL_path)\n",
    "sys.path.append(DCLL_path + 'decolle')\n",
    "import snn_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a215deda-7228-42cd-ae33-5e5bbe0b29f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DeepR_path = '/home/gb21/Code/SNNs/DEEP-R/'\n",
    "import sys, os\n",
    "sys.path.append(DeepR_path)\n",
    "import deepR_torch as deepR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ad6f263-a287-4814-82a9-26e48b01280c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "#warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4489626b-e81a-4288-8f49-719f6be5d143",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "#%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43cfaa81-4e9b-44a0-be2a-3f1fca0d59ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_SL1 = torch.nn.SmoothL1Loss()   \n",
    "cross_entropy = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def reg_loss(s, alpha=5e-7) : \n",
    "    reg_loss = 0\n",
    "    for i in range(len(s)) : \n",
    "        reg_loss += alpha*torch.sum(s[i])\n",
    "        reg_loss += alpha*torch.mean(torch.sum(s[i],dim=0)**2)\n",
    "        \n",
    "    return reg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c2ca0a7-f561-4b4f-a2de-59c32cd461bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_grad(model) : \n",
    "    for n, p in model.named_parameters() : \n",
    "        if p.grad is not None : \n",
    "            if (p.grad == 0).all() : \n",
    "                print(f'{n}, Zero Grad')\n",
    "        elif p.requires_grad : print(f'{n}, None Grad')\n",
    "            \n",
    "def decision_making(rt, decison) : \n",
    "    try : \n",
    "        deciding_ag = int(decision)\n",
    "        rt = rt[:, deciding_ag, ...]\n",
    "    except ValueError : \n",
    "        if decision == 'max' : \n",
    "            rt = torch.stack([max_decision(r)[0] for r in rt])\n",
    "        elif decision == 'sum' : \n",
    "            rt = rt[:, 0, ...] + rt[:, 1, ...]\n",
    "    return rt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf0a5e2-02ed-4a6b-ae61-3f9061e02fec",
   "metadata": {},
   "source": [
    "## MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d8cb91-3878-4c46-b6e7-b616f273f740",
   "metadata": {},
   "source": [
    "### Loading MNIST data as spiketrains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a96105-632b-45cb-9ec3-056a14bd439a",
   "metadata": {},
   "source": [
    "The following function will load the MNIST dataset using torchvision modules. It will download and pre-pre-process the data for faster usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27ed1e1a-e8c4-4f24-8516-046e2c45d713",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_train = snn_utils.get_mnist_loader(256, Nparts=50, train=True, data_path='mnist/')\n",
    "gen_test = snn_utils.get_mnist_loader(256, Nparts=50, train=False, data_path='mnist/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e1baf7-238a-410c-946e-5d26139980e6",
   "metadata": {},
   "source": [
    "Because MNIST is an image, we need to transform it into a spiketrain. The function __image2spiketrain__ in snn_utils takes case of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f0fba44-605c-47b3-bbfc-07b8534117fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = True\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "batch_size = 256\n",
    "T = 50\n",
    "burnin = 10\n",
    "train_kwargs = {'batch_size': batch_size}\n",
    "if use_cuda:\n",
    "    cuda_kwargs = {'num_workers': 0,\n",
    "                   'pin_memory': True,\n",
    "                   'shuffle': True}\n",
    "    train_kwargs.update(cuda_kwargs)\n",
    "    \n",
    "def iter_mnist(train = True, batchsize=batch_size, Nparts=10, T=T, max_rate=20, pool=False, double=False):\n",
    "    if double : \n",
    "        double_dataset = DoubleMNIST(asym=True, train=train)\n",
    "        subset = torch.utils.data.Subset(double_dataset, np.arange(len(double_dataset)//Nparts))\n",
    "        gen = torch.utils.data.DataLoader(subset, **train_kwargs)\n",
    "        n_classes = 100\n",
    "    else : \n",
    "        gen = snn_utils.get_mnist_loader(batchsize, Nparts=Nparts, train=train, data_path='mnist/')\n",
    "        n_classes = 10\n",
    "        \n",
    "    datait = iter(gen)\n",
    "    for raw_input, raw_labels in datait:\n",
    "        scale_factor = 2\n",
    "        size = lambda input :(input.shape[2]//scale_factor, input.shape[3]//scale_factor)\n",
    "        if pool : \n",
    "            raw_input = torch.nn.functional.interpolate(raw_input, size = size(raw_input), mode = 'bilinear')\n",
    "            \n",
    "        data, labels1h = snn_utils.image2spiketrain(raw_input, raw_labels, max_duration=T, gain=max_rate, n_classes=n_classes)\n",
    "        data_t = torch.FloatTensor(data)\n",
    "        labels_t = torch.Tensor(labels1h)\n",
    "        #print(labels1h)\n",
    "        \n",
    "        yield data_t, labels_t \n",
    "            \n",
    "def get_double_data(data, label, task='parity_digits') : \n",
    "    idxs = np.arange(data.shape[1])\n",
    "    np.random.shuffle(idxs)\n",
    "    double_data = torch.stack([data, data[:, idxs, ...]], axis=1)\n",
    "    \n",
    "    target = label.argmax(-1)\n",
    "    double_target = 10*target + target[:, idxs, ...]\n",
    "    \n",
    "    double_target, n_classes = get_task_target(double_target, task)\n",
    "    \n",
    "    double_label = torch.nn.functional.one_hot(double_target, n_classes).float()\n",
    "    \n",
    "    return double_data, double_label\n",
    "\n",
    "\n",
    "def get_task(label, task) : \n",
    "    double_target = label.argmax(-1)\n",
    "    double_target, n_classes = get_task_target(double_target, task)\n",
    "    double_label = torch.nn.functional.one_hot(double_target, n_classes).float()\n",
    "    \n",
    "    return double_label, n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae279077-c386-484c-b850-d5869426ecb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'rtrl'\n",
    "online_learning = (method == 'rtrl')\n",
    "\n",
    "n_in = 784\n",
    "n_hid = 100\n",
    "n_out = 2\n",
    "layer_type = LIFLayer\n",
    "p_con = 0.25\n",
    "\n",
    "agents = [SpikingAgent(n_in, [n_hid], n_out, str(n), method=method) for n in range(2)]\n",
    "sparse_con = (torch.ones((2, 2)) - torch.eye(2))*p_con\n",
    "\n",
    "community = SpikingCommunity(agents, sparse_con).to(device)\n",
    "\n",
    "lr = 5e-7\n",
    "\n",
    "optimizer_agents = torch.optim.Adamax(community.agents.parameters(), lr=lr, betas=[0., .95])\n",
    "\n",
    "model = community\n",
    "\n",
    "deepR_params = l1, gdnoise, lr, gamma, cooling = 1e-5, 0, lr, 0.99, 0.99\n",
    "#gdnoise = lr*1e-6\n",
    "deepR_params_dict = {'l1' : l1, 'gdnoise' : gdnoise, 'lr' : lr, 'gamma' : gamma, 'cooling' : cooling}\n",
    "optimizer_connections = torch.optim.Adam(community.connections.parameters(), lr=lr, betas=[0., .95])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc586123-ef98-4399-856c-2dab59f98c7f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-ab0680a89434>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'n' is not defined"
     ]
    }
   ],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401dd098-5ca8-4944-80af-90770723327f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data, label = next(iter_mnist(double= True))\n",
    "data_d, double_label_d = data.to(device), label.to(device)\n",
    "label_d, _ = get_task(double_label_d, 'parity_digits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bbec0e-b9a2-42ae-910e-6e54b948bbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "double_target = double_label_d.argmax(-1)\n",
    "target = label_d.argmax(-1)\n",
    "digits = get_digits(double_target)\n",
    "(digits[0] == target).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a05ea52-c72d-4814-a02e-9624e7241b12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "make_dot(loss_tv, dict(community.named_parameters())).render('graphs/SNNCom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c947fc-dfc5-4d9f-8492-560d3edcfcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = community.agents[0]\n",
    "optimizer_agents = torch.optim.Adamax(model.parameters(), lr=1e-8, betas=[0., .95])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c4f740-95dd-410c-a99a-c47ada0451c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "community.agents[0].LIF_layers[0].sg_function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629753d1-a650-4a6a-b5b4-b01a42615296",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec679a6f-b292-48fd-9181-03753515861c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 49 , Loss: 3.304, Acc: 0.786| Test : Loss: 4.554, Acc: 0.534: 100%|██████████| 50/50 [31:23<00:00, 37.67s/it]\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 50\n",
    "task = 'parity'\n",
    "decision = 'max'\n",
    "save_graph = False\n",
    "check_gradients = False\n",
    "\n",
    "pbar = tqdm(range(n_epochs), desc='Training : ')\n",
    "            \n",
    "nb_reconnect = []\n",
    "deciding_agents = []\n",
    "\n",
    "train_accs, train_losses = [], []\n",
    "test_accs, test_losses = [], []\n",
    "\n",
    "descs = ['', '']\n",
    "            \n",
    "com_training = (type(model) is SpikingCommunity)\n",
    "\n",
    "if com_training : \n",
    "    thetas_list, sparsity_list = [], []\n",
    "    for c in model.connections.values() : \n",
    "        if type(c) is LIFLayerConnect : \n",
    "            c = c.base_layer\n",
    "        if type(c) is deepR.Sparse_Connect : \n",
    "            con  = c\n",
    "            thetas_list.append(con.thetas[0])\n",
    "            sparsity_list.append(con.sparsity_list[0])\n",
    "            \n",
    "        else : \n",
    "            thetas_list, sparsity_list = [], []\n",
    "        \n",
    "for epoch in pbar : \n",
    "    \n",
    "    #Training :\n",
    "    for batch_idx, (data, label) in enumerate(iter_mnist(double=True)):\n",
    "            \n",
    "        model.train()\n",
    "        data_d = data.to(device)\n",
    "        label_d = label.to(device)\n",
    "        \n",
    "        if com_training : \n",
    "            #data_d, label_d = get_double_data(data_d, label_d, task=task)\n",
    "            label_d, _ = get_task(label_d, task)\n",
    "            ''\n",
    "        \n",
    "        model.init(data_d, burnin=burnin)\n",
    "        \n",
    "        loss_hist = 0\n",
    "        readout = 0\n",
    "        for n in range(burnin, T) : \n",
    "            rt, (st, ut) = model.forward(data_d[n])  \n",
    "            \n",
    "            if com_training : \n",
    "                rt = decision_making(rt, decision)                        \n",
    "            \n",
    "            loss_tv = torch.sum(torch.stack([loss_SL1(r, label_d[n]) for r in rt]))\n",
    "            \n",
    "            #loss_tv = cross_entropy(rt, label_d[n].argmax(-1))\n",
    "            if online_learning : \n",
    "                if batch_idx == 0 and save_graph : make_dot(loss_tv, dict(community.named_parameters())).render('graphs/SNNCom_Online')\n",
    "                loss_tv.backward()\n",
    "                optimizer_agents.step()\n",
    "                if n != burnin and check_gradients :\n",
    "                    check_grad(model)\n",
    "                optimizer_agents.zero_grad()\n",
    "                \n",
    "                if com_training : \n",
    "                    nb_new_con = step_connections_spiking(model, optimizer_connections, False, thetas_list,\n",
    "                                                      sparsity_list, deepR_params_dict=deepR_params_dict)\n",
    "                    nb_reconnect.append(nb_new_con.cpu().data.item())\n",
    "                    optimizer_connections.zero_grad()\n",
    "                    \n",
    "            loss_hist += loss_tv\n",
    "            readout += rt[-1]\n",
    "        \n",
    "        if not online_learning : \n",
    "            if batch_idx == 0 and save_graph : make_dot(loss_hist, dict(community.named_parameters())).render('graphs/SNNCom_Offline')\n",
    "            loss_hist.backward()\n",
    "            optimizer_agents.step()\n",
    "            if check_gradients : check_grad(model)\n",
    "            optimizer_agents.zero_grad()\n",
    "            \n",
    "            if com_training : \n",
    "                nb_new_con = step_connections_spiking(model, optimizer_connections, False, thetas_list,\n",
    "                                                      sparsity_list, deepR_params_dict=deepR_params_dict)\n",
    "                nb_reconnect.append(nb_new_con.cpu().data.item())\n",
    "                optimizer_connections.zero_grad()\n",
    "            \n",
    "            \n",
    "                    \n",
    "        acc = (readout.argmax(axis=-1)==label_d[-1].argmax(axis=-1)).float()\n",
    "        train_accs.append(acc.mean().cpu().item())\n",
    "        train_losses.append(loss_hist.cpu().item())\n",
    "\n",
    "        descs[0] = str('Train Epoch: {} , Loss: {:.3f}, Acc: {:.3f}'.format(\n",
    "                            epoch, loss_hist.item(), acc.mean().item()))\n",
    "        \n",
    "        pbar.set_description(descs[0]+descs[1])\n",
    "        \n",
    "    #Testing : \n",
    "    with torch.no_grad() : \n",
    "        for batch_idx, (data, label) in enumerate(iter_mnist(train=False, double=True)):\n",
    "\n",
    "            data_d = data.to(device)\n",
    "            label_d = label.to(device)\n",
    "\n",
    "            if com_training : \n",
    "                #data_d, label_d = get_double_data(data_d, label_d, task=task)\n",
    "                label_d, _ = get_task(label_d, task)\n",
    "                ''\n",
    "\n",
    "            model.init(data_d, burnin=burnin)\n",
    "\n",
    "            loss_hist = 0\n",
    "            readout = 0\n",
    "            for n in range(burnin, T) : \n",
    "\n",
    "                rt, (st, ut) = model.forward(data_d[n])  \n",
    "                if com_training : \n",
    "                    rt = rt[:, 1, ...]\n",
    "                    #rt = torch.stack([max_decision(r)[0] for r in rt])\n",
    "                    #rt = rt[:, 0, ...] + rt[:, 1, ...]\n",
    "                    #if len(deciding_ags) == batch_size : deciding_agents.append(deciding_ags.cpu().data)\n",
    "\n",
    "                loss_tv = torch.sum(torch.stack([loss_SL1(r, label_d[n]) for r in rt]))\n",
    "                loss_hist += loss_tv\n",
    "                readout += rt[-1] \n",
    "\n",
    "\n",
    "            acc = (readout.argmax(axis=-1)==label_d[-1].argmax(axis=-1)).float()\n",
    "\n",
    "            descs[1] = str('| Test : Loss: {:.3f}, Acc: {:.3f}'.format(\n",
    "                                loss_hist.item(), acc.mean().item()))\n",
    "            \n",
    "            test_accs.append(acc.mean().cpu().item())\n",
    "            test_losses.append(loss_hist.cpu().item())\n",
    "\n",
    "            pbar.set_description(descs[0]+descs[1])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "530ae8f0-690d-4a4b-859a-8fac6f05e643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleDict(\n",
       "  (01): LIFLayerConnect(\n",
       "    (base_layer): Sparse_Connect(\n",
       "      (thetas): ParameterList(  (0): Parameter containing: [torch.cuda.FloatTensor of size 100x100 (GPU 0)])\n",
       "      (weight): ParameterList(  (0): Parameter containing: [torch.cuda.FloatTensor of size 100x100 (GPU 0)])\n",
       "      (signs): ParameterList(  (0): Parameter containing: [torch.cuda.FloatTensor of size 100x100 (GPU 0)])\n",
       "    )\n",
       "  )\n",
       "  (10): LIFLayerConnect(\n",
       "    (base_layer): Sparse_Connect(\n",
       "      (thetas): ParameterList(  (0): Parameter containing: [torch.cuda.FloatTensor of size 100x100 (GPU 0)])\n",
       "      (weight): ParameterList(  (0): Parameter containing: [torch.cuda.FloatTensor of size 100x100 (GPU 0)])\n",
       "      (signs): ParameterList(  (0): Parameter containing: [torch.cuda.FloatTensor of size 100x100 (GPU 0)])\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "community.connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1099ce-a294-461d-9475-e1754ffa90a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_0(state) : \n",
    "    checks = []\n",
    "    if type(state) is list : \n",
    "        for s in state : \n",
    "            checks.append(check_0(s))\n",
    "    else : \n",
    "        for s in state : \n",
    "            check = (s.data == 0).all()\n",
    "            checks.append(check)\n",
    "            \n",
    "    return checks\n",
    "\n",
    "check_0(community.agents_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb71ed65-316d-41dc-8cf1-25646cd04030",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007cda19-ce67-4685-8b31-a8df53c87bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686a77ed-9a47-49fd-8d62-c4da3710285f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(torch.mean(torch.stack(deciding_agents[299::300]).float(), axis=-1).cpu())\n",
    "plt.title(f'Mean = {torch.mean(torch.stack(deciding_agents[299::300]).float())}')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4c205d-6d17-450c-a1ff-987da4d861bc",
   "metadata": {},
   "source": [
    "## Randman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d75e8bd-0da1-40dd-903e-6e38938df157",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = True\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "batch_size = 256\n",
    "\n",
    "randman_params = nb_classes, nb_units, T, nb_samples, nb_spikes = 5, 50, 50, 288, 5\n",
    "randman_data = make_spiking_dataset_torch(nb_classes=nb_classes, nb_units=nb_units, nb_spikes=nb_spikes, nb_steps=T, dim_manifold=1, nb_samples=nb_samples//nb_classes, classification=True, model_type='dcll')\n",
    "nb_total = nb_samples*nb_classes\n",
    "train_data, test_data = split_data(randman_data, p=0.89)\n",
    "train_loader, test_loader = generator_samples_randman(train_data, batchsize=64), generator_samples_randman(test_data, batchsize=64)\n",
    "data, target = next(iter(train_loader))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
